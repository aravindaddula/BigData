{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8240c-6b10-428d-8325-cd018bc43043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks perform in this Notebook taken from various platforms like linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a13efaa-887d-4f07-b20f-573642a0ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 19:45:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"GlobalSparkSession\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffca24-1230-4acc-a61e-a831c0c5335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task - The dataset includes company_id, company_name, and multiple date columns representing the stock prices of the company. \n",
    "The task is to generate a new dataset with only three columns, \"company_name\", \"Date\", \"Stock_price\".\n",
    "You have to unpivot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c78978-e557-4a77-8a35-d2105fbc62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "|c_id|        Company_Name|2024-02-01|2024-02-02|2024-02-03|2024-02-04|2024-02-05|\n",
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "|   1| Quantum Innovations|      12.5|      14.5|      16.5|      18.5|      20.5|\n",
      "|   2|   Stellar Solutions|      14.5|      16.5|      18.5|      20.5|      22.5|\n",
      "|   3|     Nebula Dynamics|      16.5|      18.5|      20.5|      22.5|      24.5|\n",
      "|   4|  Fusion Enterprises|      18.5|      20.5|      22.5|      24.5|      26.5|\n",
      "|   5|Celestial Technol...|      20.5|      22.5|      24.5|      26.5|      28.5|\n",
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Quantum Innovations\", 12.5, 14.5, 16.5, 18.5, 20.5),\n",
    "    (2, \"Stellar Solutions\", 14.5, 16.5, 18.5, 20.5, 22.5),\n",
    "    (3, \"Nebula Dynamics\", 16.5, 18.5, 20.5, 22.5, 24.5),\n",
    "    (4, \"Fusion Enterprises\", 18.5, 20.5, 22.5, 24.5, 26.5),\n",
    "    (5, \"Celestial Technologies\", 20.5, 22.5, 24.5, 26.5, 28.5),\n",
    "]\n",
    "columns=[\"c_id\",\"Company_Name\",\"2024-02-01\",\"2024-02-02\",\"2024-02-03\",\"2024-02-04\",\"2024-02-05\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c7f5b0-d8a3-4543-ae63-12a29cd3ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+-----------+\n",
      "|c_id|        Company_Name|     Dates|Stock_Price|\n",
      "+----+--------------------+----------+-----------+\n",
      "|   1| Quantum Innovations|2024-02-01|       12.5|\n",
      "|   1| Quantum Innovations|2024-02-02|       14.5|\n",
      "|   1| Quantum Innovations|2024-02-03|       16.5|\n",
      "|   1| Quantum Innovations|2024-02-04|       18.5|\n",
      "|   1| Quantum Innovations|2024-02-05|       20.5|\n",
      "|   2|   Stellar Solutions|2024-02-01|       14.5|\n",
      "|   2|   Stellar Solutions|2024-02-02|       16.5|\n",
      "|   2|   Stellar Solutions|2024-02-03|       18.5|\n",
      "|   2|   Stellar Solutions|2024-02-04|       20.5|\n",
      "|   2|   Stellar Solutions|2024-02-05|       22.5|\n",
      "|   3|     Nebula Dynamics|2024-02-01|       16.5|\n",
      "|   3|     Nebula Dynamics|2024-02-02|       18.5|\n",
      "|   3|     Nebula Dynamics|2024-02-03|       20.5|\n",
      "|   3|     Nebula Dynamics|2024-02-04|       22.5|\n",
      "|   3|     Nebula Dynamics|2024-02-05|       24.5|\n",
      "|   4|  Fusion Enterprises|2024-02-01|       18.5|\n",
      "|   5|Celestial Technol...|2024-02-01|       20.5|\n",
      "|   4|  Fusion Enterprises|2024-02-02|       20.5|\n",
      "|   5|Celestial Technol...|2024-02-02|       22.5|\n",
      "|   4|  Fusion Enterprises|2024-02-03|       22.5|\n",
      "+----+--------------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "dates_col=[\"Dates\"]\n",
    "dates_data=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df=spark.createDataFrame(dates_data,dates_col)\n",
    "resultset=df.crossJoin(dates_df)\n",
    "final_result=resultset.withColumn(\"Stock_Price\",\n",
    "                      when(col(\"Dates\")=='2024-02-01',col('2024-02-01'))\n",
    "                     .when(col(\"Dates\")=='2024-02-02',col('2024-02-02'))\n",
    "                     .when(col(\"Dates\")=='2024-02-03',col('2024-02-03'))\n",
    "                     .when(col(\"Dates\")=='2024-02-04',col('2024-02-04'))\n",
    "                     .when(col(\"Dates\")=='2024-02-05',col('2024-02-05'))\n",
    "                      )\n",
    "# use sparksql to select required columns\n",
    "final_result.select(\"c_id\",\"Company_Name\",\"Dates\",\"Stock_Price\").show()\n",
    "# dates_df.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43dd18e0-f795-4176-95b7-a81dd614e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the above task using SparkSQL\n",
    "# create temp sql table for values from dates_df dataframe\n",
    "columns2=[\"c_id\",\"Company_Name\",\"feb1\",\"feb2\",\"feb3\",\"feb4\",\"feb5\"]\n",
    "df2=spark.createDataFrame(data,columns2)\n",
    "temp_values=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df.createOrReplaceTempView(\"temp_values1\")\n",
    "df2.createOrReplaceTempView(\"temp_data1\")\n",
    "# df2.show()\n",
    "results=spark.sql(f\"select c_id,Company_Name,{'Dates'},case Dates when '{'2024-02-01'}' then feb1 when '{'2024-02-02'}' then feb2 when '{'2024-02-03'}' then feb3 when '{'2024-02-04'}' then feb4 else feb5 end as Stock_Price from temp_values1 cross join temp_data1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2712566-a7df-4872-ab4e-9f07ebc8d2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f60f57-cfa7-43e2-b613-e560afab7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results into parque file\n",
    "# results.write.parquet('/home/BigData/Files/stock_price.parquet')\n",
    "results.write.csv('/home/BigData/Files/stock_price.csv')\n",
    "\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "110f85c1-03f6-4204-b049-0c48fa81753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------------------------+\n",
      "|s_id|s_name|all_books                           |\n",
      "+----+------+------------------------------------+\n",
      "|101 |Mark  |White Tiger;Bhagwad Gita            |\n",
      "|102 |Ria   |The Fountainhead;The Seceret History|\n",
      "|103 |Loi   |The Fountainhead                    |\n",
      "+----+------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task :we have a dataset representing books issued by students, and we want to aggregate the books for each student.\n",
    "# Multiple books should be shown separated by \";\"\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "book_issued = [\n",
    "    (101 ,'Mark' , \"White Tiger\"),\n",
    "    (102 ,'Ria' , \"The Fountainhead\"),\n",
    "    (102 ,'Ria' , \"The Seceret History\"),\n",
    "    (101 ,'Mark' , \"Bhagwad Gita\"),\n",
    "    (103 ,'Loi' , \"The Fountainhead\"),\n",
    "    ]\n",
    "books_schema=['s_id','s_name','book_names']\n",
    "books_df=spark.createDataFrame(book_issued,books_schema)\n",
    "gp=books_df.groupBy(\"s_id\",\"s_name\")\\\n",
    ".agg(F.concat_ws(';',F.collect_list(\"book_names\")).alias(\"all_books\"))\\\n",
    ".show(truncate=False)\n",
    "# print(type(gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01fdab6-07c9-4a0d-afb2-df9377968f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+\n",
      "| id|        Name|Amount|\n",
      "+---+------------+------+\n",
      "|  1|  Aditya Sen|  NULL|\n",
      "|  4|  D K aditya|  NULL|\n",
      "|  3|      Mark T|   100|\n",
      "|  2|Bikramaditya|   200|\n",
      "|  6|         Eli|   300|\n",
      "|  5|       Danny|   500|\n",
      "+---+------------+------+\n",
      "\n",
      "+---+------------+------+\n",
      "| id|        Name|Amount|\n",
      "+---+------------+------+\n",
      "|  5|       Danny|   500|\n",
      "|  6|         Eli|   300|\n",
      "|  2|Bikramaditya|   200|\n",
      "|  3|      Mark T|   100|\n",
      "|  1|  Aditya Sen|  NULL|\n",
      "|  4|  D K aditya|  NULL|\n",
      "+---+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.'Arranging a column with null values in ascending or descending order.'\n",
    "\n",
    "Task - Sort the salary column of the dataframe :\n",
    "\n",
    "'''\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , None),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" ,None),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 300),\n",
    "    ]\n",
    "cust_schema=StructType([\n",
    "    StructField(\"id\",IntegerType(),False), #is Nullable\n",
    "    StructField(\"Name\",StringType(),False),\n",
    "    StructField(\"Amount\",IntegerType(),True)\n",
    "])\n",
    "cust_df=spark.createDataFrame(data,cust_schema)\n",
    "# print(cust_df)\n",
    "# cust_df.show()\n",
    "# a) in ascending order with NULL values at the top\n",
    "cust_df.orderBy(\"Amount\").show()\n",
    "# b) in ascending order with NULL values at the bottom\n",
    "\n",
    "# c) in descending order with NULL values at the top\n",
    "\n",
    "# d) in descending order with NULL values at the bottom\n",
    "cust_df.orderBy(F.desc(\"Amount\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a45e1c-b50e-4950-b4e5-ae0972b0520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|Order_id|         Order_date|Order_date2|\n",
      "+--------+-------------------+-----------+\n",
      "|       1|2024-02-01 00:00:00| 2024-02-01|\n",
      "|       2|2024-02-01 00:00:00| 2024-02-01|\n",
      "|       3|2024-02-03 00:00:00| 2024-02-03|\n",
      "|       4|2024-02-04 00:00:00| 2024-02-04|\n",
      "+--------+-------------------+-----------+\n",
      "\n",
      "+--------+----------+\n",
      "|Order_id|order_date|\n",
      "+--------+----------+\n",
      "|       1|2024-02-01|\n",
      "|       2|2024-02-01|\n",
      "|       3|2024-02-03|\n",
      "|       4|2024-02-04|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task - In your dataset, the \"order_date\" column includes a time component set to 00:00:00.\n",
    "Your objective is to eliminate the time portion from the column, thereby conserving storage space.\n",
    "'Remove time part from PySpark dataframe date column'\n",
    "'''\n",
    "from pyspark.sql.functions import col\n",
    "data = [(1, \"2024-02-01 00:00:00\"),\n",
    "        (2, \"2024-02-01 00:00:00\"),\n",
    "        (3, \"2024-02-03 00:00:00\"),\n",
    "        (4, \"2024-02-04 00:00:00\",)]\n",
    "columns = [\"Order_id\",\"Order_date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "# using sparksql\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "# print(df)\n",
    "removedTime = spark.sql(\"select Order_id,cast(substring(Order_date,1,10) as date) as order_date from orders\")\n",
    "# print(removedTime)\n",
    "\n",
    "# using Pyspark\n",
    "df.withColumn(\"Order_date2\",col(\"Order_date\")[0:10]).show()\n",
    "removedTime.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4aebf4-eebe-4eef-8a53-9d21bfd7d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''case insensitive/ignorant data search'\n",
    "Task - In the provided dataset, identify the rows where the 'Name' column contains the word 'aditya'.\n",
    "'''\n",
    "from pyspark.sql.functions import filter\n",
    "\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , 100),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" , 200),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 100),\n",
    "]\n",
    "columns = [\"id\",\"name\",\"Amount\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "# spark.sql(\"select id,name,Amount from users where lower(name) like '%aditya%'\").show()\n",
    "# df.where(\"lower(name) like '%aditya%'\").show()\n",
    "# df.filter(df[\"name\"]).rlike('aditya').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca43df11-b586-4757-9fa1-c6b2ed8c3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+--------------+---+------+\n",
      "|First Name|Last Name|              Type|    Department|YoE|Salary|\n",
      "+----------+---------+------------------+--------------+---+------+\n",
      "|     Aryan|    Singh|Full-time Employee|Administration|  2| 20000|\n",
      "|     Rohan|  Agarwal|            Intern|     Technical|  3|  5000|\n",
      "|      Riya|     Shah|Full-time Employee|Administration|  5| 10000|\n",
      "|      Yash|   Bhatia|Part-time Employee|     Technical|  7| 10000|\n",
      "|  Siddhant|   Khanna|Full-time Employee|    Management|  6| 20000|\n",
      "+----------+---------+------------------+--------------+---+------+\n",
      "\n",
      "+------------------+--------------+----------+---------+\n",
      "|              Type|Administration|Management|Technical|\n",
      "+------------------+--------------+----------+---------+\n",
      "|Full-time Employee|       15000.0|   20000.0|      0.0|\n",
      "|            Intern|           0.0|       0.0|   5000.0|\n",
      "|Part-time Employee|           0.0|       0.0|  10000.0|\n",
      "+------------------+--------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task - Show work group wise , average salary for each department\n",
    "# Suggestion: instead of following sql like approach follow, spark approach\n",
    "\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,when\n",
    "import pandas as pd\n",
    "spark = spark.builder.getOrCreate()\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"FirstName\",ArrayType(StringType()),True),\n",
    "    StructField(\"LastName\",ArrayType(StringType()),True),\n",
    "    StructField(\"Type\",ArrayType(StringType()),True),\n",
    "    StructField(\"Dept\",ArrayType(StringType()),True),\n",
    "    StructField(\"Yoe\",ArrayType(IntegerType()),True),\n",
    "    StructField(\"Sal\",ArrayType(IntegerType()),True)\n",
    "])\n",
    "data = {'First Name': ['Aryan', 'Rohan', 'Riya', 'Yash', 'Siddhant'],\n",
    "        'Last Name': ['Singh', 'Agarwal', 'Shah', 'Bhatia', 'Khanna'],\n",
    "        'Type': ['Full-time Employee', 'Intern', 'Full-time Employee', 'Part-time Employee', 'Full-time Employee'],\n",
    "        'Department': ['Administration', 'Technical', 'Administration', 'Technical', 'Management'],\n",
    "        'YoE': [2, 3, 5, 7, 6],\n",
    "        'Salary': [20000, 5000, 10000, 10000, 20000]\n",
    "       }\n",
    "\n",
    "pandas_df=pd.DataFrame(data)\n",
    "# print(pandas_df)\n",
    "# data1=(list(data.values()), list(data.keys()))\n",
    "# df = spark.createDataFrame(list(data.values()), list(data.keys()))\n",
    "\n",
    "df=spark.createDataFrame(pandas_df);\n",
    "# grouped_data=df.groupBy(\"Type\",\"Department\").agg(avg('Salary').alias(\"Avg_Salary\"))#,min(\"Department\").alias(\"Department\"))\n",
    "df.show()\n",
    "'''\n",
    "gd=grouped_data.withColumn(\"Administration\",when(col(\"Department\")=='Administration',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Technical\",when(col(\"Department\")=='Technical',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Management\",when(col(\"Department\")=='Management',col('Avg_Salary')).otherwise(0))\n",
    "# gd.select(\"Type\",\"Administration\",\"Management\",\"Technical\").show()\n",
    "'''\n",
    "df.groupBy(\"Type\").pivot(\"Department\").avg(\"Salary\").na.fill(0).alias(\"Avg_Sal\").show()\n",
    "\n",
    "# gd.show()\n",
    "# print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ff6af-7f54-415c-b14b-dee5ba2adcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
