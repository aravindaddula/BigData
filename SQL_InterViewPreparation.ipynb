{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6199a8-da93-4a81-819c-42099d833773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/09 22:45:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"SQL_GlobalSparkSession\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591bc06-5b5e-4a47-9040-0080041f9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💻 Write a pyspark code to convert table A value as table B.\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "schema=['key','values']\n",
    "data= [('K1',[1,1,2]),('K2',[1,2,3])]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "df1.createOrReplaceTempView(\"keyvalues\")\n",
    "df1.select(col('key'),expr(\"values[0]\").alias(\"A\"),expr(\"values[1]\").alias(\"B\"), expr(\"values[2]\").alias(\"C\")).show()\n",
    "spark.sql(\"\"\"\n",
    "    select key,values[0] as A,values[1] as B,values[2] as C from keyvalues\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45635b69-13d5-4412-95fa-68ada3f628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭 :\n",
    "Write a query that'll identify returning active users. \n",
    "A returning active user is a user that has made a second purchase within 7 days of any other of their purchases.\n",
    "Output a list of user_ids of these returning active users.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema=['user_id','item','created_at','revenue']\n",
    "\n",
    "data=[ (100, 'bread', '07-03-2020', 410),(100, 'banana' ,'13-03-2020' ,175),\n",
    "        (100, 'banana','29-03-2020', 599),(101, 'milk', '01-03-2020', 449),\n",
    "        (101, 'milk', '26-03-2020', 740),(114, 'banana', '10-03-2020', 200),\n",
    "        (114, 'biscuit', '16-03-2020', 300)\n",
    "     ]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"revenues_2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "with clean_data as(\n",
    "    select user_id,item,revenue\n",
    "    ,cast(substr(created_at,7,4)||'-'||substr(created_at,4,2)||'-'||substr(created_at,1,2) as date) as order_dt\n",
    "    from revenues_2\n",
    ")\n",
    "--FInd frequency of purchase\n",
    ",find_frequency as (\n",
    "    select user_id,item,revenue,order_dt\n",
    "    --,coalesce(lag(order_dt) over(partition by user_id order by order_dt) ,current_date()) as prev_date\n",
    "    ,datediff(order_dt,lag(order_dt) over(partition by user_id order by order_dt)) as diff_days\n",
    "    from clean_data\n",
    ")\n",
    "select * \n",
    "from find_frequency\n",
    "where diff_days <= 7\n",
    "\"\"\")#.show()\n",
    "# ans 100,114\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select date('07-03-2020') as dt\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c830f-f664-4f6b-8dd2-00d9a21ec976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭 :\n",
    "Given a table of purchases by date, calculate the month-over-month 𝐩𝐞𝐫𝐜𝐞𝐧𝐭𝐚𝐠𝐞 𝐜𝐡𝐚𝐧𝐠𝐞 𝐢𝐧 𝐫𝐞𝐯𝐞𝐧𝐮𝐞.\n",
    "The output should include the year-month date (YYYY-MM) and percentage change, rounded to the 2nd decimal point,\n",
    "and sorted from the beginning of the year to the end of the year.\n",
    "\"\"\"\n",
    "schema=['id','created_at','value','purchase_id']\n",
    "\n",
    "data=[(1,'01-01-2019',172692,43), (2,'05-01-2019',177194,36),\n",
    "        (3,'06-02-2019',116948,56), (4,'10-02-2019',162515,29),\n",
    "        (5,'14-02-2019',120741,30), (6,'22-03-2019',151688,34),\n",
    "        (7,'26-03-2019',1002327,44)\n",
    "     ]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"revenues\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "with sum_amt (\n",
    "    select\n",
    "     substring(created_at,4,10) as year_month\n",
    "    ,sum(value) as curr_month\n",
    "    from revenues\n",
    "    group by substring(created_at,4,10)\n",
    "    )\n",
    ",get_prevmonth as(\n",
    "    select year_month,curr_month\n",
    "        ,coalesce(lag(curr_month) over(order by year_month),0) as prev_month\n",
    "        from sum_amt\n",
    "    )\n",
    "select year_month,curr_month,prev_month\n",
    "    ,case when prev_month !=0 then round((curr_month-prev_month)/prev_month * 100,2)\n",
    "    else null end as revenue_percentage\n",
    "    from get_prevmonth\n",
    "    \n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ca724-f7b1-4a75-89d0-c46dcaa3d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭 :\n",
    "Write an SQL query to find how many users visited the bank and didn't do any transactions,\n",
    "how many visited the bank and did one transaction and so on.\n",
    "\"\"\"\n",
    "\n",
    "visits_schema=['user_id','visit_date']\n",
    "transactions_schema=['user_id','transaction_date','amount']\n",
    "\n",
    "visits=[('1', '2020-01-01'),('2', '2020-01-02'),('12', '2020-01-01'),('19', '2020-01-03')\n",
    "        ,('1', '2020-01-02'),('2', '2020-01-03'),('1', '2020-01-04'),('7', '2020-01-11'),\n",
    "        ('9', '2020-01-25'),('8', '2020-01-28')]\n",
    "\n",
    "transactions=[('1', '2020-01-02', '120'), ('2', '2020-01-03', '22'),('7', '2020-01-11', '232'),('1', '2020-01-04', '7')\n",
    "             ,('9', '2020-01-25', '33'),('9', '2020-01-25', '66'),('8', '2020-01-28', '1'),('9', '2020-01-25', '99')\n",
    "             ]\n",
    "df1=spark.createDataFrame(visits,visits_schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"visits\")\n",
    "df2=spark.createDataFrame(transactions,transactions_schema)\n",
    "# df2.show()\n",
    "df2.createOrReplaceTempView(\"transactions\")\n",
    "spark.sql(\"\"\"\n",
    "with find_0 as (\n",
    "    select \n",
    "    0 as t_count,\n",
    "    count(v.user_id) as v_count\n",
    "    \n",
    "    from visits as v\n",
    "    left join transactions as t\n",
    "    on v.user_id=t.user_id and v.visit_date=t.transaction_date\n",
    "    where t.user_id is null\n",
    ")\n",
    ",find_13 as(\n",
    "select t_count as no_of_trans,count(1) as t_count \n",
    "from(\n",
    "    select t.user_id,t.transaction_date,count(1) as t_count\n",
    "    from transactions as t\n",
    "    group by t.user_id,t.transaction_date\n",
    ")\n",
    "group by t_count\n",
    ")\n",
    "select * from find_0\n",
    "union\n",
    "select * from find_13\n",
    "\"\"\")#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d803c-f5f5-4967-9a05-1368c3951187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#thoughtworks SQL Interview Question for Data Engineer , Data Analyst and Data Science role.\n",
    "\n",
    "𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭 : A 𝐟𝐫𝐢𝐞𝐧𝐝𝐬𝐡𝐢𝐩 between a pair of friends x and y is strong if x and y have at least three common friends.\n",
    "Write an SQL query to find all the strong friendships.\n",
    "Note that the result table should not contain duplicates with user1_id < user2_id ( To have the friendship user1_id < user2_id)\n",
    "\"\"\"\n",
    "\n",
    "schema=['user1_id','user2_id']\n",
    "data=[('1', '2'),('1', '3'),('2', '3'),('1', '4'),('2', '4'),('1', '5')\n",
    "      ,('2', '5'),('1', '7'), ('3', '7'), ('1', '6'),('3', '6'),('2', '6')]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"Friendship\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    F1.user1_id,\n",
    "    F1.user2_id\n",
    "FROM\n",
    "    Friendship F1\n",
    "JOIN\n",
    "    Friendship F2 ON F1.user1_id = F2.user1_id AND F1.user2_id = F2.user2_id AND F1.user1_id < F1.user2_id\n",
    "JOIN\n",
    "    Friendship F3 ON F1.user1_id = F3.user1_id AND F1.user2_id = F3.user2_id AND F1.user1_id < F3.user1_id AND F1.user2_id < F3.user2_id\n",
    "    AND F2.user1_id = F3.user1_id AND F2.user2_id = F3.user2_id\n",
    "ORDER BY\n",
    "    F1.user1_id,\n",
    "    F1.user2_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e2d36-b4ad-48bf-8af6-14cbdb2f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Pending\n",
    "𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭:\n",
    "𝐘𝐨𝐮 𝐚𝐫𝐞 𝐠𝐢𝐯𝐞𝐧 𝐭𝐡𝐞 𝐩𝐫𝐢𝐜𝐞 𝐨𝐟 𝐞𝐚𝐜𝐡 𝐬𝐤𝐮 𝐰𝐡𝐞𝐧𝐞𝐯𝐞𝐫 𝐭𝐡𝐞𝐫𝐞 𝐢𝐬 𝐚 𝐜𝐡𝐚𝐧𝐠𝐞 𝐢𝐧 𝐩𝐫𝐢𝐜𝐞.\n",
    "Write an SQL query 𝐭𝐨 𝐟𝐢𝐧𝐝 𝐭𝐡𝐞 𝐩𝐫𝐢𝐜𝐞 𝐚𝐭 𝐭𝐡𝐞 𝐬𝐭𝐚𝐫𝐭 𝐨𝐟 𝐞𝐚𝐜𝐡 𝐦𝐨𝐧𝐭𝐡 & 𝐜𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐝𝐢𝐟𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐟𝐫𝐨𝐦 𝐭𝐡𝐞 𝐩𝐫𝐞𝐯𝐢𝐨𝐮𝐬 𝐦𝐨𝐧𝐭𝐡'𝐬 𝐬𝐭𝐚𝐫𝐭 𝐝𝐚𝐭𝐞.\n",
    "\"\"\"\n",
    "schema=['sku_id','price_date','price']\n",
    "\n",
    "data = [(1,'2023-01-01',10)\n",
    ",(1,'2023-02-15',15)\n",
    ",(1,'2023-03-03',18)\n",
    ",(1,'2023-03-27',15)\n",
    ",(1,'2023-04-06',20)]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"sku\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select *--,price-prev_price as price_diff \n",
    "    from(\n",
    "    select *,coalesce(lag(price) over(partition by sku_id order by price_date ),0) as prev_price\n",
    "    ,price-coalesce(lag(price) over(partition by sku_id order by price_date ),0) as prev_pricediff\n",
    "    from sku --order by price\n",
    "    )\n",
    "\"\"\")\n",
    "#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28366fb-7e0d-471f-8f2c-90856f617b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the stores which either sell both tea and coffee  or coffee and jam\n",
    "\"\"\"\n",
    "\n",
    "schema=['store_id','store_name','product']\n",
    "data=[\n",
    "(1,'bb','tea'),\n",
    "(1,'bb','coffee'),\n",
    "(2,'dmart','coffee'),\n",
    "(3,'more','tea'),\n",
    "(4,'reliance','tea'),\n",
    "(4,'reliance','coffee'),\n",
    "(4,'reliance','jam'),\n",
    "(5,'b mart','coffee'),\n",
    "(5,'b mart','jam')\n",
    "]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"stores\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select store_id,store_name--,product\n",
    ",max(case when product='tea' then product else null end) as tea\n",
    ",max(case when product='coffee' then product else null end) as coffee\n",
    ",max(case when product='jam' then product else null end) as jam\n",
    "\n",
    "from stores\n",
    "group by store_id,store_name\n",
    "having (tea='tea' and coffee='coffee') or (jam='jam' and coffee='coffee')\n",
    "\n",
    "\"\"\")\n",
    "spark.sql(\"\"\" \n",
    "    SELECT store_id, store_name\n",
    "    FROM stores\n",
    "    WHERE product IN ('tea', 'coffee', 'jam')\n",
    "    GROUP BY store_id, store_name\n",
    "    HAVING COUNT(DISTINCT product) >= 2\n",
    "\"\"\")# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81283991-95aa-4553-8d62-fc3ff81bf804",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Asked in indiumsoft L2 Round\n",
    "return the customers who place the order within 12 days\n",
    "'''\n",
    "data=[\n",
    "(1,'2021/12/31','123a','b1'),\n",
    "(2,'2022/01/11','123a','b2'),\n",
    "(3,'2023/01/01','123a','b3'),\n",
    "(4,'2023/02/02','123a','b4'),\n",
    "(5,'2023/01/01','346b','b5'),\n",
    "(6,'2023/02/02','346b','b6'),\n",
    "(7,'2023/02/14','346b','b7'),\n",
    "(8,'2023/02/17','346b','b7')\n",
    "]\n",
    "schema=['Orderid','Orderdt','custid','Endloc']\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "# way 1\n",
    "spark.sql(\"\"\"\n",
    "  select *,datediff(orderdt,prevdt) as days\n",
    "    from (\n",
    "        select Orderid,to_date(Orderdt,'yyyy/MM/dd') as orderdt\n",
    "            ,custid,Endloc\n",
    "            ,coalesce(lag(to_date(Orderdt,'yyyy/MM/dd')) over(partition by custid order by custid),orderdt ) as prevdt\n",
    "        from orders \n",
    "        order by custid,orderdt\n",
    "        )\n",
    "        where datediff(orderdt,prevdt) <=12\n",
    "\"\"\")#.show()\n",
    "\n",
    "# way 2\n",
    "spark.sql(\"\"\"\n",
    "select * from (\n",
    "select *\n",
    "--,datediff(orderdt,lag(orderdt) over(partition by custid order by custid) ) as days\n",
    ",to_date(orderdt,'yyyy/mm/dd') as currentdt, lag(to_date(orderdt,'yyyy/mm/dd')) over(partition by custid order by custid) as prevdt\n",
    "\n",
    "from orders )\n",
    "--where days <=12\n",
    "\"\"\")\\\n",
    "# .show()\n",
    "\n",
    "# print(type(df.Orderdt))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55ec1e-1bb8-4c50-964e-f04335fb99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Asked in synechron\n",
    "Metro Train table is ordered by turn in the example for simplicity.\n",
    "In the example Rajkumar(id 105). Shruthitid 103) and Ravikiran (id 106) will enter \n",
    "the Metro_Train as their weight sum is 250-350+400=1000\n",
    "Ravikiran (id/106) is the last person to fir in the Metro Train because he has the last turn in these three people.\n",
    "\"\"\"\n",
    "schema=['id','name','weight','turn_id']\n",
    "data=[(105,'Rajkumar',250,1)\n",
    ",(103,'Shruthi',350,2)\n",
    ",(106,'Ravikiran',400,3)\n",
    ",(102,'Tejuswini',200,4)\n",
    ",(104,'Ravikiran',1175,5)\n",
    ",(101,'Surya',500,6)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73bd12-e720-4c59-8959-0135b4c13a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name some of the frameworks that SCALA supports\n",
    "how to skip first few rows from a datafile in pyspark\n",
    "ETL vs ELT\n",
    "sheedusele single mode duster with 64dores and 512/68 memory.or should go for smaller nodes such as eight of 8 cores and 64 GB memory?\n",
    "Which one is better in which scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a2838-590e-4fe0-9ccd-36cb68ab71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "𝑪𝒉𝒂𝒍𝒍𝒆𝒏𝒈𝒆 : Get the name of the 𝐡𝐢𝐠𝐡𝐞𝐬𝐭 𝐬𝐚𝐥𝐚𝐫𝐲 𝐚𝐧𝐝 𝐥𝐨𝐰𝐞𝐬𝐭 𝐬𝐚𝐥𝐚𝐫𝐲 employee name in 𝐞𝐚𝐜𝐡 𝐝𝐞𝐩𝐚𝐫𝐭𝐦𝐞𝐧𝐭.\n",
    "If the salary is same then return the name of the employee whose name comes first in 𝐥𝐞𝐱𝐢𝐜𝐨𝐠𝐫𝐚𝐩𝐡𝐢𝐜𝐚𝐥 𝐨𝐫𝐝𝐞𝐫.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_data = [\n",
    "    ('Siva',1,30000),('Ravi',2,40000),('Prasad',1,50000),\n",
    "    ('Arun',1,30000),('Sai',2,20000)\n",
    "]\n",
    "\n",
    "emp_schema = \"emp_name string , dep_id int , salary long\"\n",
    "\n",
    "# Creating the dataframe\n",
    "df = spark.createDataFrame(data = emp_data , schema = emp_schema).createOrReplaceTempView(\"depts\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select dep_id\n",
    "    ,max((case when minSal=1 then emp_name end)) as minEmp\n",
    "    ,max((case when maxSal=1 then emp_name end)) as maxEmp\n",
    "    from ( select dep_id,emp_name,salary\n",
    "    ,row_number() over(partition by dep_id order by salary,emp_name) as minSal\n",
    "    ,row_number() over(partition by dep_id order by salary desc,emp_name) as maxSal\n",
    "    from depts \n",
    "    )\n",
    "    group by dep_id\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
