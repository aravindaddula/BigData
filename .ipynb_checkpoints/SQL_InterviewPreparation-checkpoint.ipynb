{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6199a8-da93-4a81-819c-42099d833773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 19:36:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"SQL_GlobalSparkSession\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591bc06-5b5e-4a47-9040-0080041f9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’» Write a pyspark code to convert table A value as table B.\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "schema=['key','values']\n",
    "data= [('K1',[1,1,2]),('K2',[1,2,3])]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "df1.createOrReplaceTempView(\"keyvalues\")\n",
    "df1.select(col('key'),expr(\"values[0]\").alias(\"A\"),expr(\"values[1]\").alias(\"B\"), expr(\"values[2]\").alias(\"C\")).show()\n",
    "spark.sql(\"\"\"\n",
    "    select key,values[0] as A,values[1] as B,values[2] as C from keyvalues\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45635b69-13d5-4412-95fa-68ada3f628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ :\n",
    "Write a query that'll identify returning active users. \n",
    "A returning active user is a user that has made a second purchase within 7 days of any other of their purchases.\n",
    "Output a list of user_ids of these returning active users.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema=['user_id','item','created_at','revenue']\n",
    "\n",
    "data=[ (100, 'bread', '07-03-2020', 410),(100, 'banana' ,'13-03-2020' ,175),\n",
    "        (100, 'banana','29-03-2020', 599),(101, 'milk', '01-03-2020', 449),\n",
    "        (101, 'milk', '26-03-2020', 740),(114, 'banana', '10-03-2020', 200),\n",
    "        (114, 'biscuit', '16-03-2020', 300)\n",
    "     ]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"revenues_2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "with clean_data as(\n",
    "    select user_id,item,revenue\n",
    "    ,cast(substr(created_at,7,4)||'-'||substr(created_at,4,2)||'-'||substr(created_at,1,2) as date) as order_dt\n",
    "    from revenues_2\n",
    ")\n",
    "--FInd frequency of purchase\n",
    ",find_frequency as (\n",
    "    select user_id,item,revenue,order_dt\n",
    "    --,coalesce(lag(order_dt) over(partition by user_id order by order_dt) ,current_date()) as prev_date\n",
    "    ,datediff(order_dt,lag(order_dt) over(partition by user_id order by order_dt)) as diff_days\n",
    "    from clean_data\n",
    ")\n",
    "select * \n",
    "from find_frequency\n",
    "where diff_days <= 7\n",
    "\"\"\")#.show()\n",
    "# ans 100,114\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select date('07-03-2020') as dt\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c830f-f664-4f6b-8dd2-00d9a21ec976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ :\n",
    "Given a table of purchases by date, calculate the month-over-month ğ©ğğ«ğœğğ§ğ­ğšğ ğ ğœğ¡ğšğ§ğ ğ ğ¢ğ§ ğ«ğğ¯ğğ§ğ®ğ.\n",
    "The output should include the year-month date (YYYY-MM) and percentage change, rounded to the 2nd decimal point,\n",
    "and sorted from the beginning of the year to the end of the year.\n",
    "\"\"\"\n",
    "schema=['id','created_at','value','purchase_id']\n",
    "\n",
    "data=[(1,'01-01-2019',172692,43), (2,'05-01-2019',177194,36),\n",
    "        (3,'06-02-2019',116948,56), (4,'10-02-2019',162515,29),\n",
    "        (5,'14-02-2019',120741,30), (6,'22-03-2019',151688,34),\n",
    "        (7,'26-03-2019',1002327,44)\n",
    "     ]\n",
    "\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"revenues\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "with sum_amt (\n",
    "    select\n",
    "     substring(created_at,4,10) as year_month\n",
    "    ,sum(value) as curr_month\n",
    "    from revenues\n",
    "    group by substring(created_at,4,10)\n",
    "    )\n",
    ",get_prevmonth as(\n",
    "    select year_month,curr_month\n",
    "        ,coalesce(lag(curr_month) over(order by year_month),0) as prev_month\n",
    "        from sum_amt\n",
    "    )\n",
    "select year_month,curr_month,prev_month\n",
    "    ,case when prev_month !=0 then round((curr_month-prev_month)/prev_month * 100,2)\n",
    "    else null end as revenue_percentage\n",
    "    from get_prevmonth\n",
    "    \n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ca724-f7b1-4a75-89d0-c46dcaa3d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ :\n",
    "Write an SQL query to find how many users visited the bank and didn't do any transactions,\n",
    "how many visited the bank and did one transaction and so on.\n",
    "\"\"\"\n",
    "\n",
    "visits_schema=['user_id','visit_date']\n",
    "transactions_schema=['user_id','transaction_date','amount']\n",
    "\n",
    "visits=[('1', '2020-01-01'),('2', '2020-01-02'),('12', '2020-01-01'),('19', '2020-01-03')\n",
    "        ,('1', '2020-01-02'),('2', '2020-01-03'),('1', '2020-01-04'),('7', '2020-01-11'),\n",
    "        ('9', '2020-01-25'),('8', '2020-01-28')]\n",
    "\n",
    "transactions=[('1', '2020-01-02', '120'), ('2', '2020-01-03', '22'),('7', '2020-01-11', '232'),('1', '2020-01-04', '7')\n",
    "             ,('9', '2020-01-25', '33'),('9', '2020-01-25', '66'),('8', '2020-01-28', '1'),('9', '2020-01-25', '99')\n",
    "             ]\n",
    "df1=spark.createDataFrame(visits,visits_schema)\n",
    "# df1.show()\n",
    "df1.createOrReplaceTempView(\"visits\")\n",
    "df2=spark.createDataFrame(transactions,transactions_schema)\n",
    "# df2.show()\n",
    "df2.createOrReplaceTempView(\"transactions\")\n",
    "spark.sql(\"\"\"\n",
    "with find_0 as (\n",
    "    select \n",
    "    0 as t_count,\n",
    "    count(v.user_id) as v_count\n",
    "    \n",
    "    from visits as v\n",
    "    left join transactions as t\n",
    "    on v.user_id=t.user_id and v.visit_date=t.transaction_date\n",
    "    where t.user_id is null\n",
    ")\n",
    ",find_13 as(\n",
    "select t_count as no_of_trans,count(1) as t_count \n",
    "from(\n",
    "    select t.user_id,t.transaction_date,count(1) as t_count\n",
    "    from transactions as t\n",
    "    group by t.user_id,t.transaction_date\n",
    ")\n",
    "group by t_count\n",
    ")\n",
    "select * from find_0\n",
    "union\n",
    "select * from find_13\n",
    "\"\"\")#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d803c-f5f5-4967-9a05-1368c3951187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#thoughtworks SQL Interview Question for Data Engineer , Data Analyst and Data Science role.\n",
    "\n",
    "ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ : A ğŸğ«ğ¢ğğ§ğğ¬ğ¡ğ¢ğ© between a pair of friends x and y is strong if x and y have at least three common friends.\n",
    "Write an SQL query to find all the strong friendships.\n",
    "Note that the result table should not contain duplicates with user1_id < user2_id ( To have the friendship user1_id < user2_id)\n",
    "\"\"\"\n",
    "\n",
    "schema=['user1_id','user2_id']\n",
    "data=[('1', '2'),('1', '3'),('2', '3'),('1', '4'),('2', '4'),('1', '5')\n",
    "      ,('2', '5'),('1', '7'), ('3', '7'), ('1', '6'),('3', '6'),('2', '6')]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"Friendship\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    F1.user1_id,\n",
    "    F1.user2_id\n",
    "FROM\n",
    "    Friendship F1\n",
    "JOIN\n",
    "    Friendship F2 ON F1.user1_id = F2.user1_id AND F1.user2_id = F2.user2_id AND F1.user1_id < F1.user2_id\n",
    "JOIN\n",
    "    Friendship F3 ON F1.user1_id = F3.user1_id AND F1.user2_id = F3.user2_id AND F1.user1_id < F3.user1_id AND F1.user2_id < F3.user2_id\n",
    "    AND F2.user1_id = F3.user1_id AND F2.user2_id = F3.user2_id\n",
    "ORDER BY\n",
    "    F1.user1_id,\n",
    "    F1.user2_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e2d36-b4ad-48bf-8af6-14cbdb2f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Pending\n",
    "ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­:\n",
    "ğ˜ğ¨ğ® ğšğ«ğ ğ ğ¢ğ¯ğğ§ ğ­ğ¡ğ ğ©ğ«ğ¢ğœğ ğ¨ğŸ ğğšğœğ¡ ğ¬ğ¤ğ® ğ°ğ¡ğğ§ğğ¯ğğ« ğ­ğ¡ğğ«ğ ğ¢ğ¬ ğš ğœğ¡ğšğ§ğ ğ ğ¢ğ§ ğ©ğ«ğ¢ğœğ.\n",
    "Write an SQL query ğ­ğ¨ ğŸğ¢ğ§ğ ğ­ğ¡ğ ğ©ğ«ğ¢ğœğ ğšğ­ ğ­ğ¡ğ ğ¬ğ­ğšğ«ğ­ ğ¨ğŸ ğğšğœğ¡ ğ¦ğ¨ğ§ğ­ğ¡ & ğœğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğğ¢ğŸğŸğğ«ğğ§ğœğ ğŸğ«ğ¨ğ¦ ğ­ğ¡ğ ğ©ğ«ğğ¯ğ¢ğ¨ğ®ğ¬ ğ¦ğ¨ğ§ğ­ğ¡'ğ¬ ğ¬ğ­ğšğ«ğ­ ğğšğ­ğ.\n",
    "\"\"\"\n",
    "schema=['sku_id','price_date','price']\n",
    "\n",
    "data = [(1,'2023-01-01',10)\n",
    ",(1,'2023-02-15',15)\n",
    ",(1,'2023-03-03',18)\n",
    ",(1,'2023-03-27',15)\n",
    ",(1,'2023-04-06',20)]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"sku\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select *--,price-prev_price as price_diff \n",
    "    from(\n",
    "    select *,coalesce(lag(price) over(partition by sku_id order by price_date ),0) as prev_price\n",
    "    ,price-coalesce(lag(price) over(partition by sku_id order by price_date ),0) as prev_pricediff\n",
    "    from sku --order by price\n",
    "    )\n",
    "\"\"\")\n",
    "#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28366fb-7e0d-471f-8f2c-90856f617b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the stores which either sell both tea and coffee  or coffee and jam\n",
    "\"\"\"\n",
    "\n",
    "schema=['store_id','store_name','product']\n",
    "data=[\n",
    "(1,'bb','tea'),\n",
    "(1,'bb','coffee'),\n",
    "(2,'dmart','coffee'),\n",
    "(3,'more','tea'),\n",
    "(4,'reliance','tea'),\n",
    "(4,'reliance','coffee'),\n",
    "(4,'reliance','jam'),\n",
    "(5,'b mart','coffee'),\n",
    "(5,'b mart','jam')\n",
    "]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.createOrReplaceTempView(\"stores\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select store_id,store_name--,product\n",
    ",max(case when product='tea' then product else null end) as tea\n",
    ",max(case when product='coffee' then product else null end) as coffee\n",
    ",max(case when product='jam' then product else null end) as jam\n",
    "\n",
    "from stores\n",
    "group by store_id,store_name\n",
    "having (tea='tea' and coffee='coffee') or (jam='jam' and coffee='coffee')\n",
    "\n",
    "\"\"\")\n",
    "spark.sql(\"\"\" \n",
    "    SELECT store_id, store_name\n",
    "    FROM stores\n",
    "    WHERE product IN ('tea', 'coffee', 'jam')\n",
    "    GROUP BY store_id, store_name\n",
    "    HAVING COUNT(DISTINCT product) >= 2\n",
    "\"\"\")# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81283991-95aa-4553-8d62-fc3ff81bf804",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Asked in indiumsoft L2 Round\n",
    "return the customers who place the order within 12 days\n",
    "'''\n",
    "data=[\n",
    "(1,'2021/12/31','123a','b1'),\n",
    "(2,'2022/01/11','123a','b2'),\n",
    "(3,'2023/01/01','123a','b3'),\n",
    "(4,'2023/02/02','123a','b4'),\n",
    "(5,'2023/01/01','346b','b5'),\n",
    "(6,'2023/02/02','346b','b6'),\n",
    "(7,'2023/02/14','346b','b7'),\n",
    "(8,'2023/02/17','346b','b7')\n",
    "]\n",
    "schema=['Orderid','Orderdt','custid','Endloc']\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "# way 1\n",
    "spark.sql(\"\"\"\n",
    "  select *,datediff(orderdt,prevdt) as days\n",
    "    from (\n",
    "        select Orderid,to_date(Orderdt,'yyyy/MM/dd') as orderdt\n",
    "            ,custid,Endloc\n",
    "            ,coalesce(lag(to_date(Orderdt,'yyyy/MM/dd')) over(partition by custid order by custid),orderdt ) as prevdt\n",
    "        from orders \n",
    "        order by custid,orderdt\n",
    "        )\n",
    "        where datediff(orderdt,prevdt) <=12\n",
    "\"\"\")#.show()\n",
    "\n",
    "# way 2\n",
    "spark.sql(\"\"\"\n",
    "select * from (\n",
    "select *\n",
    "--,datediff(orderdt,lag(orderdt) over(partition by custid order by custid) ) as days\n",
    ",to_date(orderdt,'yyyy/mm/dd') as currentdt, lag(to_date(orderdt,'yyyy/mm/dd')) over(partition by custid order by custid) as prevdt\n",
    "\n",
    "from orders )\n",
    "--where days <=12\n",
    "\"\"\")\\\n",
    "# .show()\n",
    "\n",
    "# print(type(df.Orderdt))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55ec1e-1bb8-4c50-964e-f04335fb99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Asked in synechron\n",
    "Metro Train table is ordered by turn in the example for simplicity.\n",
    "In the example Rajkumar(id 105). Shruthitid 103) and Ravikiran (id 106) will enter \n",
    "the Metro_Train as their weight sum is 250-350+400=1000\n",
    "Ravikiran (id/106) is the last person to fir in the Metro Train because he has the last turn in these three people.\n",
    "\"\"\"\n",
    "schema=['id','name','weight','turn_id']\n",
    "data=[(105,'Rajkumar',250,1)\n",
    ",(103,'Shruthi',350,2)\n",
    ",(106,'Ravikiran',400,3)\n",
    ",(102,'Tejuswini',200,4)\n",
    ",(104,'Ravikiran',1175,5)\n",
    ",(101,'Surya',500,6)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73bd12-e720-4c59-8959-0135b4c13a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name some of the frameworks that SCALA supports\n",
    "how to skip first few rows from a datafile in pyspark\n",
    "ETL vs ELT\n",
    "sheedusele single mode duster with 64dores and 512/68 memory.or should go for smaller nodes such as eight of 8 cores and 64 GB memory?\n",
    "Which one is better in which scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a2838-590e-4fe0-9ccd-36cb68ab71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğ‘ªğ’‰ğ’‚ğ’ğ’ğ’†ğ’ğ’ˆğ’† : Get the name of the ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ¬ğšğ¥ğšğ«ğ² ğšğ§ğ ğ¥ğ¨ğ°ğğ¬ğ­ ğ¬ğšğ¥ğšğ«ğ² employee name in ğğšğœğ¡ ğğğ©ğšğ«ğ­ğ¦ğğ§ğ­.\n",
    "If the salary is same then return the name of the employee whose name comes first in ğ¥ğğ±ğ¢ğœğ¨ğ ğ«ğšğ©ğ¡ğ¢ğœğšğ¥ ğ¨ğ«ğğğ«.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_data = [\n",
    "    ('Siva',1,30000),('Ravi',2,40000),('Prasad',1,50000),\n",
    "    ('Arun',1,30000),('Sai',2,20000)\n",
    "]\n",
    "\n",
    "emp_schema = \"emp_name string , dep_id int , salary long\"\n",
    "\n",
    "# Creating the dataframe\n",
    "df = spark.createDataFrame(data = emp_data , schema = emp_schema).createOrReplaceTempView(\"depts\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select dep_id\n",
    "    ,max((case when minSal=1 then emp_name end)) as minEmp\n",
    "    ,max((case when maxSal=1 then emp_name end)) as maxEmp\n",
    "    from ( select dep_id,emp_name,salary\n",
    "    ,row_number() over(partition by dep_id order by salary,emp_name) as minSal\n",
    "    ,row_number() over(partition by dep_id order by salary desc,emp_name) as maxSal\n",
    "    from depts \n",
    "    )\n",
    "    group by dep_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31afba8-07b9-498b-82b5-4351879cc05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, n_part: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Challenge:- Read the third quarter (25%) of emp_data table.\n",
    "\"\"\"\n",
    "\n",
    "emp_data = [(\"Alice\", 28),\n",
    "(\"Bob\", 35),(\"Harvey\",45),(\"Donna\",45),\n",
    "(\"Charlie\", 42),\n",
    "(\"David\", 25),\n",
    "(\"Eva\", 31),\n",
    "(\"Frank\", 38),\n",
    "(\"Grace\", 45),\n",
    "(\"Henry\", 29)]\n",
    "\n",
    "emp_schema = \"name string , age int\"\n",
    "\n",
    "df = spark.createDataFrame(data = emp_data , schema = emp_schema).createOrReplaceTempView(\"emp_data\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select * from (\n",
    "select name,age\n",
    ",ntile(4) over(order by name) as n_part\n",
    "from emp_data\n",
    ")\n",
    "--where n_part=3\n",
    "\"\"\")#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06efa1af-4bea-41da-ac00-9dffa47f84f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, City: string, State: string, Lat_N: double, Long_W: double]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Lat_N\", DoubleType(), True),\n",
    "    StructField(\"Long_W\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 'New York', 'NY', 40.7128, -74.006),\n",
    "    (2, 'Los Angeles', 'CA', 34.0522, -118.2437),\n",
    "    (3, 'Albany', 'IL', 41.8781, -87.6298),\n",
    "    (4, 'East China', 'TX', 29.7604, -95.3698),\n",
    "    (5, 'East China', 'AZ', 33.4484, -112.074),\n",
    "    (6, 'Philadelphia', 'PA', 39.9526, -75.1652),\n",
    "    (7, 'San Antonio', 'TX', 29.4241, -98.4936),\n",
    "    (8, 'San Diego', 'CA', 32.7157, -117.1611),\n",
    "    (9, 'Oakfield', 'TX', 32.7767, -96.797),\n",
    "    (10, 'San Jose', 'CA', 37.3382, -121.8863)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=schema).createOrReplaceTempView(\"cities\")\n",
    "# Filter city names that start with vowels\n",
    "# spark.sql(\"select * from cities where City like 'A%' or City like 'E%' or City like 'I%' or City like 'O%'\")\\\n",
    "spark.sql(\"select * from cities where City rlike '^[AEIOUaeiou]'\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c9f7c1-d860-4668-90ea-faf9b2b2357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[student_id: string, course_id: string, grade: string, rn: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ : Write a SQL query to find the highest grade with its corresponding course for each student.\n",
    "In case of a tie, you should find the course with the smallest course_id.\n",
    "The output must be sorted by increasing student_id.\n",
    "\n",
    "select a.*,b.course_id\n",
    "    from get_grades as a\n",
    "    join student_grades as b\n",
    "    on a.student_id=b.student_id \n",
    "    where max_marks=b.grade\n",
    "\n",
    "\"\"\"\n",
    "schema=[\"student_id\",\"course_id\",\"grade\"]\n",
    "data=[   ('2', '2', '95'),('2', '3', '95')\n",
    "        ,('1', '1', '90'),('1', '2', '99')\n",
    "        ,('3', '1', '80'),('3', '2', '75')\n",
    "        ,('3', '3', '82')]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=schema).createOrReplaceTempView(\"student_grades\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select * from (\n",
    "    select\n",
    "    student_id,course_id,grade\n",
    "    ,row_number() over(partition by student_id order by grade desc,course_id ) as rn\n",
    "    from student_grades\n",
    "    ) where rn=1\n",
    "\"\"\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d92cd2a-e2a4-4d9f-b454-555897a020ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|team_id|team_name|\n",
      "+-------+---------+\n",
      "|      1|      CSK|\n",
      "|      4|      RCB|\n",
      "|      6|      KKR|\n",
      "+-------+---------+\n",
      "\n",
      "+------------+------------+---------------+---------------+\n",
      "|home_team_id|away_team_id|home_team_goals|away_team_goals|\n",
      "+------------+------------+---------------+---------------+\n",
      "|           1|           4|              0|              1|\n",
      "|           1|           6|              3|              3|\n",
      "|           4|           1|              5|              2|\n",
      "|           6|           1|              0|              0|\n",
      "+------------+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "teams_schema=[\"team_id\",\"team_name\"]\n",
    "Matches_schema=[\"home_team_id\",\"away_team_id\",\"home_team_goals\",\"away_team_goals\"]\n",
    "teams =[('1', 'CSK'),('4', 'RCB'),('6', 'KKR')]\n",
    "matches=[('1', '4', '0', '1'),('1', '6', '3', '3')\n",
    "        ,('4', '1', '5', '2'),('6', '1', '0', '0')]\n",
    "\n",
    "spark.createDataFrame(data=teams, schema=teams_schema).createOrReplaceTempView(\"teams\")\n",
    "spark.createDataFrame(data=matches, schema=Matches_schema).createOrReplaceTempView(\"matches\")\n",
    "\"\"\"\n",
    "ğğ«ğ¨ğ›ğ¥ğğ¦ ğ’ğ­ğšğ­ğğ¦ğğ§ğ­ : Write an SQL query to report the statistics of the league.\n",
    "The statistics should be built using the played matches where the winning team gets ğ­ğ¡ğ«ğğ ğ©ğ¨ğ¢ğ§ğ­ğ¬ and the losing team gets no points.\n",
    "If a match ends with a draw, both teams get one point.\n",
    "Return the result table in descending order by points.\n",
    "If two or more teams have the same points, order them in descending order by goal_diff. \n",
    "If there is still a tie, order them by team_name in lexicographical order.\n",
    "\n",
    "Output format: team_name,matches_played,points,goal_for,goal_points,goal_diff\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select * from teams\n",
    "\"\"\")\\\n",
    ".show()\n",
    "spark.sql(\"\"\"\n",
    "select * from matches\n",
    "\"\"\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ddd9e5-21db-4b64-ad7f-8a2b87cf5a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: int, emp_name: string, emp_salary: float, mgr_salary: float]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Find the employees who earn more than their managers.\n",
    "\"\"\"\n",
    "data = [\n",
    "    (1, 'John Doe', 60000.0, 3),\n",
    "    (2, 'Jane Smith', 95000.0, 3),\n",
    "    (3, 'Tom Johnson', 90000.0, 4),\n",
    "    (4, 'Alice Brown', 120000.0, 5),\n",
    "    (5, 'Bob White', 80000.0, 4),\n",
    "    (6, 'Eva Davis', 65000.0, 3),\n",
    "# Add more tuples as needed\n",
    "]\n",
    "schema=\"emp_id int,emp_name string,salary float,mgr_id int\"\n",
    "spark.createDataFrame(data, schema).createOrReplaceTempView(\"employees\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select e.emp_id as emp_id,e.emp_name,e.salary as emp_salary,m.salary as mgr_salary--,e.mgr_id,m.emp_name as mgr_name\n",
    "    from employees as e\n",
    "    inner join employees as m\n",
    "    on e.mgr_id=m.emp_id\n",
    "    and e.salary >=m.salary\n",
    "\"\"\")#.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ed0b037-6f5e-4c03-9e25-53ca1b2cc2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------------+\n",
      "|   id|total_rides|succesfull_rides|\n",
      "+-----+-----------+----------------+\n",
      "|dri_1|          5|               2|\n",
      "|dri_2|          2|               0|\n",
      "+-----+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write a query to find total rides and profit rides by each driver.\n",
    "Profit ride is when the end location of the current ride is the same as the start location of the next ride.\n",
    "\n",
    "It is not necessary that the end time of the current ride should be the same as the start time of the next ride to qualify as a profit ride.\n",
    "\n",
    "Bonus point if you solve it without using lead and lag functions.\n",
    "\n",
    "script :\n",
    "\"\"\"\n",
    "drivers_schema=\"id string,start_time string, end_time string, start_loc string, end_loc string\";\n",
    "\n",
    "drivers = [('dri_1', '09:00', '09:30', 'a','b'),('dri_1', '09:30', '10:30', 'b','c')\n",
    "        ,('dri_1','11:00','11:30', 'd','e'),('dri_1', '12:00', '12:30', 'f','g'),('dri_1', '13:30', '14:30', 'c','h')\n",
    "        ,('dri_2', '12:15', '12:30', 'f','g'),('dri_2', '13:30', '14:30', 'c','h')\n",
    "          ]\n",
    "spark.createDataFrame(drivers, drivers_schema).createOrReplaceTempView(\"drivers\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select id,count(start_time) as total_rides,sum(next_ride) as succesfull_rides\n",
    "    from (\n",
    "    select distinct s.id,s.start_loc,s.start_time\n",
    "    ,case when s.end_loc=n.start_loc then 1 else 0 end as next_ride\n",
    "    from drivers as s\n",
    "    left join drivers as n\n",
    "    on s.end_loc=n.start_loc\n",
    "    order by s.start_time\n",
    "    ) group by id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40170d28-d066-4be0-8e6c-c17d336b6ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     9100.0|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|department_id|dept_avg_salary|\n",
      "+-------------+---------------+\n",
      "|          300|         6500.0|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I would have solved hundreds of SQL problems based on the employee and department table in the last 10-12 years.\n",
    "But this problem is really a tricky one.\n",
    "\n",
    "We need to obtain a list of departments with an average salary lower than the overall average salary of the company.\n",
    "\n",
    "However, when calculating the company's average salary, you must exclude the salaries of the department you are comparing it with.\n",
    "\n",
    "For instance, when comparing the average salary of the HR department with the company's average, the HR department's salaries \n",
    "shouldn't be taken into consideration for the calculation of the company's average salary.\n",
    "\"\"\"\n",
    "emp_schema=\"emp_id int,emp_name string,department_id int,salary int,manager_id int,emp_age int\"\n",
    "\n",
    "emp=[ (1, 'Ankit', 100,10000, 4, 39),(2,'Mohit', 100, 15000, 5, 48)\n",
    "    , (3, 'Vikas', 100, 10000,4,37),(4,'Rohit', 100, 5000, 2, 16)\n",
    "    , (5, 'Mudit', 200, 12000, 6,55),(6,'Agam', 200, 12000,2, 14)\n",
    "    , (7, 'Sanjay', 200, 9000, 2,13),(8,'Ashish', 200,5000,2,12)\n",
    "    , (9, 'Mukesh',300,6000,6,51),(10,'Rakesh',300,7000,6,50)\n",
    "]\n",
    "\n",
    "spark.createDataFrame(emp, emp_schema).createOrReplaceTempView(\"employees\")\n",
    "spark.sql(\"\"\"\n",
    "   select avg(salary) from employees   \n",
    "\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "    department_id,avg(salary) as dept_avg_salary\n",
    "    from employees\n",
    "    group by department_id\n",
    "    having dept_avg_salary <= (select avg(salary) from employees)\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
