{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a090f4-fb3d-4be3-9c6b-75a03e831a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file covers basic steps to get started with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce10357-88b1-4ce4-89ab-48f5cec4d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e5ebd5-9516-4a81-98be-f86f4e34aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Read and display data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('first').getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35),(\"Tony\",50)]\n",
    "df=spark.createDataFrame(data,[\"Name\",\"Age\"])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1643b9c0-ae31-4ebb-8832-ba53107af397",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Task 2 : Filtering Data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mfilter(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAge\u001b[49m\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3125\u001b[0m     )\n\u001b[1;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Age'"
     ]
    }
   ],
   "source": [
    "# Task 2 : Filtering Data\n",
    "df.filter(df.Age>30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbda4fe-f506-433f-8caa-13abbbe3eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 25|    1|\n",
      "| 30|    1|\n",
      "| 35|    1|\n",
      "| 50|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task : Grouping and Aggregation\n",
    "df.groupBy(df.Age).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e661-0d51-4f35-8f53-397dbe68fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f1df3d-3e2d-4acc-8733-2deb96a8bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Occupation|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|  Engineer|\n",
      "|    Bob| 30|    Doctor|\n",
      "|Charlie| 35|   Teacher|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task Joining dataframes\n",
    "data2 = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\"), (\"Charlie\", \"Teacher\")]\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Occupation\"])\n",
    "df.join(df2,\"Name\").show()\n",
    "# Alice,(25,Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3fb845-bc02-4f33-9fb2-7701f566543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task : Handling Missing Data or remove the record that contain null value\n",
    "data_with_null = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 35)]\n",
    "df_with_null = spark.createDataFrame(data_with_null, [\"Name\", \"Age\"])\n",
    "df_with_null.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605880-2d7d-4622-84a1-9975bcefa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "994e84d5-7a89-4225-ac68-30d3e83c3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|UpperName|\n",
      "+-------+---+---------+\n",
      "|  Alice| 25|    ALICE|\n",
      "|    Bob| 30|      BOB|\n",
      "|Charlie| 35|  CHARLIE|\n",
      "|   Tony| 50|     TONY|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task: User-Defined Functions (UDFs): \n",
    "# Convert & Create upperCase column for Name column\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "upper_udf=udf(lambda name:name.upper())\n",
    "df.withColumn(\"UpperName\",upper_udf(df[\"Name\"])).show()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f17b39e-2f7c-4eaa-9a0f-173ca08cd0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   Name|Age|Row_num|\n",
      "+-------+---+-------+\n",
      "|  Alice| 25|      1|\n",
      "|    Bob| 30|      2|\n",
      "|Charlie| 35|      3|\n",
      "|   Tony| 50|      4|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window Functions : generate rownumber based on Age class\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "orderBySpec=Window.orderBy(\"Age\")\n",
    "df.withColumn(\"Row_num\",row_number().over(orderBySpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b9e117-7d50-4cc5-b2b1-3e1aef57ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+\n",
      "|employee_name|department|sum(salary)|\n",
      "+-------------+----------+-----------+\n",
      "|      Michael|     Sales|       4600|\n",
      "|        James|     Sales|       6000|\n",
      "|        Maria|   Finance|       3000|\n",
      "|       Robert|     Sales|       4100|\n",
      "|        Scott|   Finance|       3300|\n",
      "|          Jen|   Finance|       3900|\n",
      "|         Jeff| Marketing|       3000|\n",
      "|         Saif|     Sales|       4100|\n",
      "|        Kumar| Marketing|       2000|\n",
      "+-------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Groupby on multiple columns\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)\n",
    "df.groupBy(\"employee_name\", \"department\").sum(\"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6364f24e-69fd-469a-a930-5346b4c8df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+\n",
      "|firstname|middlename|lastname|regno|sex|\n",
      "+---------+----------+--------+-----+---+\n",
      "|James    |          |Smith   |36636|M  |\n",
      "|Michael  |Rose      |        |40288|M  |\n",
      "|Robert   |          |Williams|42114|M  |\n",
      "|Maria    |Anne      |Jones   |39192|F  |\n",
      "|Jen      |Mary      |Brown   |NULL |F  |\n",
      "+---------+----------+--------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define nested structType\n",
    "# Defining schema using nested StructType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),None,\"F\",-1)\n",
    "  ]\n",
    "structSchema=StructType([\n",
    "    StructField(\"fullname\",\n",
    "            StructType([\n",
    "                StructField(\"firstname\",StringType(),True),\n",
    "                StructField(\"middlename\",StringType(),True),\n",
    "                StructField(\"lastname\",StringType(),True)\n",
    "    ])),\n",
    "    StructField(\"regno\",StringType(),True),\n",
    "    StructField(\"sex\",StringType(),True),\n",
    "    StructField(\"salary\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=structureData,schema=structSchema)\n",
    "df.select(\"fullname.firstname\",\"fullname.middlename\",\"fullname.lastname\",\"regno\",\"sex\").show(truncate=False)\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac070764-9e7e-437a-b314-e8e5b32b3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and write into parquet file\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = schema)\n",
    "'''\n",
    "Write the data into different file formats into HDFS Location\n",
    "'''\n",
    "# df.write.parquet(\"output.parquet\", compression=\"snappy\")\n",
    "# df.write.csv('Files/output.csv')\n",
    "df.write.orc('output.orc')\n",
    "df.write.json('output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5acf115-e493-4e88-9e1f-0789424a99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+--------+\n",
      "|employee_name|department|salary|Test_Col|\n",
      "+-------------+----------+------+--------+\n",
      "|        James|     Sales|  3000|  Salary|\n",
      "|      Michael|     Sales|  4600|  Salary|\n",
      "|       Robert|     Sales|  4100|  Salary|\n",
      "|        Maria|   Finance|  3000|  Salary|\n",
      "+-------------+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add constant columns into dataframe using lit()\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.select(\"employee_name\", \"department\", \"salary\",lit(\"Salary\").alias(\"Test_Col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c987c-8942-46b3-bbf2-91f685d44fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
