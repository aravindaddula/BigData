{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8240c-6b10-428d-8325-cd018bc43043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks perform in this Notebook taken from various platforms like linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db089f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a13efaa-887d-4f07-b20f-573642a0ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"GlobalSparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdffca24-1230-4acc-a61e-a831c0c5335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Alternative Configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .appName(\"Spark Interview Preparation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c78978-e557-4a77-8a35-d2105fbc62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task - The dataset includes company_id, company_name, and multiple date columns representing the stock prices of the company. \n",
    "The task is to generate a new dataset with only three columns, \"company_name\", \"Date\", \"Stock_price\".\n",
    "You have to unpivot the data.\n",
    "\"\"\"\n",
    "data = [\n",
    "    (1, \"Quantum Innovations\", 12.5, 14.5, 16.5, 18.5, 20.5),\n",
    "    (2, \"Stellar Solutions\", 14.5, 16.5, 18.5, 20.5, 22.5),\n",
    "    (3, \"Nebula Dynamics\", 16.5, 18.5, 20.5, 22.5, 24.5),\n",
    "    (4, \"Fusion Enterprises\", 18.5, 20.5, 22.5, 24.5, 26.5),\n",
    "    (5, \"Celestial Technologies\", 20.5, 22.5, 24.5, 26.5, 28.5),\n",
    "]\n",
    "columns=[\"c_id\",\"Company_Name\",\"2024-02-01\",\"2024-02-02\",\"2024-02-03\",\"2024-02-04\",\"2024-02-05\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41c7f5b0-d8a3-4543-ae63-12a29cd3ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c_id: bigint, Company_Name: string, Dates: string, Stock_Price: double]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "dates_col=[\"Dates\"]\n",
    "dates_data=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df=spark.createDataFrame(dates_data,dates_col)\n",
    "resultset=df.crossJoin(dates_df)\n",
    "final_result=resultset.withColumn(\"Stock_Price\",\n",
    "                      when(col(\"Dates\")=='2024-02-01',col('2024-02-01'))\n",
    "                     .when(col(\"Dates\")=='2024-02-02',col('2024-02-02'))\n",
    "                     .when(col(\"Dates\")=='2024-02-03',col('2024-02-03'))\n",
    "                     .when(col(\"Dates\")=='2024-02-04',col('2024-02-04'))\n",
    "                     .when(col(\"Dates\")=='2024-02-05',col('2024-02-05'))\n",
    "                      )\n",
    "# use sparksql to select required columns\n",
    "final_result.select(\"c_id\",\"Company_Name\",\"Dates\",\"Stock_Price\")\n",
    "# .show()\n",
    "# dates_df.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43dd18e0-f795-4176-95b7-a81dd614e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the above task using SparkSQL\n",
    "# create temp sql table for values from dates_df dataframe\n",
    "columns2=[\"c_id\",\"Company_Name\",\"feb1\",\"feb2\",\"feb3\",\"feb4\",\"feb5\"]\n",
    "df2=spark.createDataFrame(data,columns2)\n",
    "temp_values=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df.createOrReplaceTempView(\"temp_values1\")\n",
    "df2.createOrReplaceTempView(\"temp_data1\")\n",
    "# df2.show()\n",
    "results=spark.sql(f\"select c_id,Company_Name,{'Dates'},case Dates when '{'2024-02-01'}' then feb1 when '{'2024-02-02'}' then feb2 when '{'2024-02-03'}' then feb3 when '{'2024-02-04'}' then feb4 else feb5 end as Stock_Price from temp_values1 cross join temp_data1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2712566-a7df-4872-ab4e-9f07ebc8d2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f60f57-cfa7-43e2-b613-e560afab7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results into parque file\n",
    "# results.write.parquet('/home/BigData/Files/stock_price.parquet')\n",
    "results.write.csv('/home/BigData/Files/stock_price.csv')\n",
    "\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "110f85c1-03f6-4204-b049-0c48fa81753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task :we have a dataset representing books issued by students, and we want to aggregate the books for each student.\n",
    "# Multiple books should be shown separated by \";\"\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "book_issued = [\n",
    "    (101 ,'Mark' , \"White Tiger\"),\n",
    "    (102 ,'Ria' , \"The Fountainhead\"),\n",
    "    (102 ,'Ria' , \"The Seceret History\"),\n",
    "    (101 ,'Mark' , \"Bhagwad Gita\"),\n",
    "    (103 ,'Loi' , \"The Fountainhead\"),\n",
    "    ]\n",
    "books_schema=['s_id','s_name','book_names']\n",
    "books_df=spark.createDataFrame(book_issued,books_schema)\n",
    "gp=books_df.groupBy(\"s_id\",\"s_name\")\\\n",
    ".agg(F.concat_ws(';',F.collect_list(\"book_names\")).alias(\"all_books\"))\\\n",
    "# .show(truncate=False)\n",
    "# print(type(gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d01fdab6-07c9-4a0d-afb2-df9377968f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, Name: string, Amount: int]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.'Arranging a column with null values in ascending or descending order.'\n",
    "Task - Sort the salary column of the dataframe :\n",
    "'''\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , None),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" ,None),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 300),\n",
    "    ]\n",
    "cust_schema=StructType([\n",
    "    StructField(\"id\",IntegerType(),False), #is Nullable\n",
    "    StructField(\"Name\",StringType(),False),\n",
    "    StructField(\"Amount\",IntegerType(),True)\n",
    "])\n",
    "cust_df=spark.createDataFrame(data,cust_schema)\n",
    "# print(cust_df)\n",
    "# cust_df.show()\n",
    "# a) in ascending order with NULL values at the top\n",
    "cust_df.orderBy(\"Amount\")\n",
    "# .show()\n",
    "# b) in ascending order with NULL values at the bottom\n",
    "\n",
    "# c) in descending order with NULL values at the top\n",
    "\n",
    "# d) in descending order with NULL values at the bottom\n",
    "cust_df.orderBy(F.desc(\"Amount\"))\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a45e1c-b50e-4950-b4e5-ae0972b0520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Order_id: bigint, Order_date: string, Order_date2: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Task - In your dataset, the \"order_date\" column includes a time component set to 00:00:00.\n",
    "Your objective is to eliminate the time portion from the column, thereby conserving storage space.\n",
    "'Remove time part from PySpark dataframe date column'\n",
    "'''\n",
    "from pyspark.sql.functions import col,slice\n",
    "data = [(1, \"2024-02-01 00:00:00\"),\n",
    "        (2, \"2024-02-01 00:00:00\"),\n",
    "        (3, \"2024-02-03 00:00:00\"),\n",
    "        (4, \"2024-02-04 00:00:00\",)]\n",
    "columns = [\"Order_id\",\"Order_date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "# df.select(df.Order_date[1:10]).show()\n",
    "# using sparksql\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "# print(df)\n",
    "removedTime = spark.sql(\"select Order_id,cast(substring(Order_date,1,10) as date) as order_date from orders\")\n",
    "# print(removedTime)\n",
    "\n",
    "# using Pyspark\n",
    "df.withColumn(\"Order_date2\",col(\"Order_date\")[0:10])\n",
    "# .show()\n",
    "# removedTime.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4aebf4-eebe-4eef-8a53-9d21bfd7d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "case insensitive/ignorant data search'\n",
    "Task - In the provided dataset, identify the rows where the 'Name' column contains the word 'aditya'.\n",
    "'''\n",
    "from pyspark.sql.functions import filter\n",
    "\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , 100),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" , 200),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 100),\n",
    "]\n",
    "columns = [\"id\",\"name\",\"Amount\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "# spark.sql(\"select id,name,Amount from users where lower(name) like '%aditya%'\").show()\n",
    "# df.where(\"lower(name) like '%aditya%'\").show()\n",
    "# df.filter(df[\"name\"]).rlike('aditya').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca43df11-b586-4757-9fa1-c6b2ed8c3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+----------+---------+\n",
      "|              Type|Administration|Management|Technical|\n",
      "+------------------+--------------+----------+---------+\n",
      "|Full-time Employee|       15000.0|   20000.0|     NULL|\n",
      "|            Intern|          NULL|      NULL|   5000.0|\n",
      "|Part-time Employee|          NULL|      NULL|  10000.0|\n",
      "+------------------+--------------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task - Show work group wise , average salary for each department\n",
    "# Suggestion: instead of following sql like approach follow, spark approach\n",
    "\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,when,avg\n",
    "import pandas as pd\n",
    "spark = spark.builder.getOrCreate()\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"FirstName\",ArrayType(StringType()),True),\n",
    "    StructField(\"LastName\",ArrayType(StringType()),True),\n",
    "    StructField(\"Type\",ArrayType(StringType()),True),\n",
    "    StructField(\"Dept\",ArrayType(StringType()),True),\n",
    "    StructField(\"Yoe\",ArrayType(IntegerType()),True),\n",
    "    StructField(\"Sal\",ArrayType(IntegerType()),True)\n",
    "])\n",
    "data = {'First Name': ['Aryan', 'Rohan', 'Riya', 'Yash', 'Siddhant'],\n",
    "        'Last Name': ['Singh', 'Agarwal', 'Shah', 'Bhatia', 'Khanna'],\n",
    "        'Type': ['Full-time Employee', 'Intern', 'Full-time Employee', 'Part-time Employee', 'Full-time Employee'],\n",
    "        'Department': ['Administration', 'Technical', 'Administration', 'Technical', 'Management'],\n",
    "        'YoE': [2, 3, 5, 7, 6],\n",
    "        'Salary': [20000, 5000, 10000, 10000, 20000]\n",
    "       }\n",
    "\n",
    "pandas_df=pd.DataFrame(data)\n",
    "# print(pandas_df)\n",
    "# data1=(list(data.values()), list(data.keys()))\n",
    "# df = spark.createDataFrame(list(data.values()), list(data.keys()))\n",
    "\n",
    "df=spark.createDataFrame(pandas_df);\n",
    "grouped_data=df.groupBy(\"Type\",\"Department\").agg(avg('Salary').alias(\"Avg_Salary\"))#,min(\"Department\").alias(\"Department\"))\n",
    "# grouped_data.show()\n",
    "'''\n",
    "gd=grouped_data.withColumn(\"Administration\",when(col(\"Department\")=='Administration',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Technical\",when(col(\"Department\")=='Technical',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Management\",when(col(\"Department\")=='Management',col('Avg_Salary')).otherwise(0))\n",
    "# gd.select(\"Type\",\"Administration\",\"Management\",\"Technical\").show()\n",
    "'''\n",
    "df.groupBy(\"Type\").pivot(\"Department\").avg(\"Salary\")#.na.fill(0).alias(\"Avg_Sal\")\n",
    "# .show()\n",
    "\n",
    "# gd.show()\n",
    "# print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bf6ff6af-7f54-415c-b14b-dee5ba2adcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task - You are provided with two datasets, branch1 and branch2 , representing information about students and\n",
    "their marks in different subjects across different branches. Your goal is to combine these datasets into one final dataset.\n",
    "Missing text information should be shown as 'unknown' , and missing numerical information should be shown as -9999.\n",
    "'''\n",
    "from pyspark.sql.functions import lit\n",
    "branch1 = spark.createDataFrame([[\"Delhi\", \"Neha\", 90]], [\"Branch\", \"Student\", \"Maths_marks\"])\n",
    "branch2 = spark.createDataFrame([[\"Arav\",\"Kolkata\", 79, 83], [None,\"Kolkata\", 89, 73]],[\"Student\", \"Branch\", \"Science_marks\", \"Maths_marks\"])\n",
    "\n",
    "branch_v1=branch1.withColumn(\"Science_marks\",lit(-9999)).select(\"Student\",\"Branch\",\"Science_marks\",\"Maths_marks\")\n",
    "union_data=branch_v1.union(branch2).selectExpr(\"Branch\",\"case when Student is null then 'Unknown' else Student end as Student\",\"Maths_marks\",\"Science_marks\")\n",
    "# union_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd454d-b118-41bf-b16e-56f00af8a344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[orders: string, count: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Task - The dataset contains information about food items in bills.\n",
    "Your assignment is to determine the frequency of each food item ordered.\n",
    "'''\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,ArrayType,StringType\n",
    "from pyspark.sql.functions import explode,count\n",
    "import pandas as pd\n",
    "data = [\n",
    "(101, [\"dosa\", \"biriyani\", \"idli\"]),\n",
    "(102, [\"biriyani\", \"mineral water\"]),\n",
    "(103, [\"rice\", \"mineral water\", \"poha\"]),\n",
    "(109, [\"idli\", \"biriyani\", \"poha\"]),\n",
    "]\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"orders\",ArrayType(StringType()),True)\n",
    "])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).select(explode(\"orders\").alias(\"orders\")) #.show()\n",
    "getCount=df.groupBy(\"orders\").count()\n",
    "getCount.orderBy([\"count\"],ascending=False)#.show(truncate=False)\n",
    "# exploded=df.withColumn(\"items\",explode(df.orders))\n",
    "# exploded.show(truncate=False)\n",
    "# orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d173b71b-8fb0-4c06-a93a-23ce87eb3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized code for the above\n",
    "\n",
    "df=spark.createDataFrame(data,schema).select(explode(\"orders\").alias(\"orders\")) \\\n",
    ".groupBy(\"orders\") \\\n",
    ".count().orderBy(\"count\",ascending=False) \\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f10600e-2bb3-4f29-9f4e-5aaf68049fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task - Transform the input DataFrame into a new DataFrame , where each row represents a unique student\n",
    "and the columns include the student's name along with the marks for the \"math\" and \"eng\" subjects.\n",
    "'''\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,ArrayType,StringType\n",
    "\n",
    "data=[\n",
    "('Rudra','math',79),\n",
    "('Rudra','eng',60),\n",
    "('Shivu','math', 68),\n",
    "('Shivu','eng', 59),\n",
    "('Anu','math', 65),\n",
    "('Anu','eng',80)\n",
    "]\n",
    "schema=StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sub\",StringType(),True),\n",
    "    StructField(\"Marks\",IntegerType(),True)\n",
    "])\n",
    "df=spark.createDataFrame(data,schema)\\\n",
    "# .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f950b6-2824-45ff-bfdd-d22109dcf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Read a table data from the sources like SQL Server/Hive/datalake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5a5ff7b-63ea-4449-a1c8-f87d9bf67738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 23:39:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Question: Given a list of sales transactions with product names and their corresponding sales amounts,\n",
    "calculate the total sales for each product and store the results in a dictionary.\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "spark=SparkSession.builder.appName(\"interview_prep\").getOrCreate()\n",
    "sales = [(\"Widget\", 500), (\"Gadget\", 750), (\"Widget\", 1000), (\"Doodad\", 1200), (\"Gadget\", 600)]\n",
    "sales_df=spark.createDataFrame(sales,schema=[\"product\",\"price\"])\n",
    "sales_df=sales_df.groupBy(\"product\").agg(sum(\"price\").alias(\"sum_sal\"))\n",
    "# sales_df.show()\n",
    "newdf=sales_df.select(\"product\",\"sum_sal\")\n",
    "# newdf.show()\n",
    "#.\\\n",
    "# product_sales_dict = sales_df.to_dict()\n",
    "# print(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6afc5ed-d711-4183-b1b5-f010d62191de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 348:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Doodad': 1200, 'Widget': 1500, 'Gadget': 1350}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# above problem using RDD\n",
    "sales = [(\"Widget\", 500), (\"Gadget\", 750), (\"Widget\", 1000), (\"Doodad\", 1200), (\"Gadget\", 600)]\n",
    "rdd=sc.parallelize(sales)\n",
    "\n",
    "# you have to give column index because RDD dont have column names.\n",
    "# any transformation returns pipelined RDD\n",
    "groupData=rdd.reduceByKey(lambda x,y:x+y).collectAsMap()\n",
    "# data=groupData.collect()\n",
    "print(groupData)\n",
    "\n",
    "# print(type(data))\n",
    "# print(groupData)\n",
    "# unpacked_data = {key: list(value) for key, value in groupData.items()}\n",
    "# print(unpacked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d81ee3-b566-4ee3-812e-0c33b1d003da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Distinct count: \"+str(df2.count()))\\ndf2.show(truncate=False)\\n#Drop duplicates on selected columns\\ndropDisDF = df.dropDuplicates([\"department\",\"salary\"])\\nprint(\"Distinct count of department salary :\\n\"+str(dropDisDF.count()))\\ndropDisDF.show(truncate=False)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ways to handle duplicate data\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),(\"Michael\", \"Sales\", 4600),(\"Robert\", \"Sales\", 4100),(\"Maria\", \"Finance\", 3000), \\\n",
    " (\"James\", \"Sales\", 3000),(\"Scott\", \"Finance\", 3300),(\"Jen\", \"Finance\", 3900),(\"Jeff\", \"Marketing\", 3000), \\\n",
    " (\"Kumar\", \"Marketing\", 2000),(\"Saif\", \"Sales\", 4100)\n",
    " ]\n",
    "column= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = column)\n",
    "# df.printSchema()\n",
    "df.orderBy(\"employee_name\",ascending=True)#.show(truncate=False)\n",
    "\n",
    "#Distinct\n",
    "distinctDF = df.distinct()\n",
    "# print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.orderBy(\"employee_name\",ascending=True)#.show(truncate=False)\n",
    "#Drop duplicates\n",
    "\n",
    "df2 = df.dropDuplicates()\n",
    "df2.orderBy(\"employee_name\",ascending=True)#.show()\n",
    "\n",
    "\"\"\"\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "df2.show(truncate=False)\n",
    "#Drop duplicates on selected columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count of department salary :\n",
    "\"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90d489-dae3-4f08-a55e-038de450d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast variables examples practice\n",
    "\n",
    "spark=SparkSession.builder.appName('SparkByExample.com').getOrCreate()\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    " (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    " (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    " (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    " ]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    " return broadcastState.value[code]\n",
    "res = rdd.map(lambda a:(a[0],a[1],a[2],state_convert(a{3]))).collect()\n",
    "print(res)\n",
    "\n",
    "PySpark DataFrame Broadcast variable example\n",
    "spark=SparkSession.builder.appName('PySpark broadcastvariable').getOrCreate()\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), (\"Robert\",\"William\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "         ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastState.value[code]\n",
    "res = df.rdd.map(lambda a:(a[0],a[1],a[2],state_convert(a[3]))).toDF(column)\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a1719-8c6f-4206-9831-5a29dcd269d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. How will you merge two files – File1 and File2 – into a single DataFrame if they have different schemas?\n",
    "File -1:\n",
    "Name|Age\n",
    "Azarudeen, Shahul|25\n",
    "Michel, Clarke|26\n",
    "Virat, Kohli|28\n",
    "Andrew, Simond|37\n",
    "File -2:\n",
    "Name|Age|Gender\n",
    "Rabindra, Tagore |32|Male\n",
    "Madona, Laure | 59|Female\n",
    "Flintoff, David|12|Male\n",
    "Ammie, James| 20|Female\n",
    "Answer- import findspark\n",
    "findspark.init()\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession, types\n",
    "spark = SparkSession.builder.master(\"local\").appName('Modes of Dataframereader').getorCreate()\n",
    "sc=spark.sparkContext\n",
    "df1=spark.read.option(\"delimiter\",\"|\").csv('input.csv')\n",
    "\n",
    "df2=spark.read.option(\"delimiter\",\"|\").csv(\"input2.csv\",header=True)\n",
    "from pyspark.sql.functions import lit\n",
    "df_add=df1.withColumn(\"Gender\",lit(\"null\"))\n",
    "df_add. union(df2).show()\n",
    "For the Unionfrom pyspark.sql.types import *\n",
    "schema=StructType(\n",
    " [\n",
    " StructField(\"Name\",StringType(), True),\n",
    " StructField(\"Age\",StringType(), True),\n",
    " StructField(\"Gender\",StringType(),True),\n",
    " ]\n",
    ")\n",
    "df3=spark.read.option(\"delimiter\",\"|\").csv(\"input.csv\",header=True, schema=schema)\n",
    "df4=spark.read.option(\"delimiter\",\"|\").csv(\"input2.csv\",header=True, schema=schema)\n",
    "df3.union(df4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18769e1d-3bbf-43e1-a280-f1838e05debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. Examine the following file, which contains some corrupt/bad data. What will you do with such data, and how will\n",
    "you import them into a Spark Dataframe?\n",
    "\n",
    "\"\"\"\n",
    "Emp_no, Emp_name, Department\n",
    "101, Murugan, HealthCare\n",
    "Invalid Entry, Description: Bad Record entry\n",
    "102, Kannan, Finance\n",
    "103, Mani, IT\n",
    "Connection lost, Description: Poor Connection\n",
    "104, Pavan, HR\n",
    "Bad Record, Description: Corrupt record\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark. sql import Sparksession, types\n",
    "spark = Sparksession.builder.master(\"local\").appName(\"Modes of Dataframereader\").getorcreate()\n",
    "sc=spark. sparkContext\n",
    "from pyspark.sql.types import *\n",
    "schm structiype([\n",
    "structField(\"col_1\",stringType(), True),\n",
    "StructField(\"col_2\",stringType(), True),\n",
    "structrield(\"col\",stringtype(), True),\n",
    "])\n",
    "\n",
    "df=spark.read.option(\"mode\",\"DROPMALFORMED\").csv('input1.csv', header=True,schema=schm)\n",
    "df. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769ac59-748f-40f3-ba74-cc3c10b69533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. Consider a file containing an Education column that includes an array of elements, as shown below. Using Spark\n",
    "Dataframe, convert each element in the array to a record.\n",
    "\"\"\"\n",
    "Name| Age | EducaÈ›ion\n",
    "Azar|25| MBA,BE,HSC\n",
    "Hari|32|\n",
    "Kumar|35|ME,BE,Diploma\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, types\n",
    "spark =\n",
    "SparkSession.builder.master(\"local\").appName('scenario based').getorCreate()\n",
    "sc=spark.sparkContext\n",
    "in_df=spark.read.option(\"delimiter\",\"|\").csv(\"input4.csv\",header-True)\n",
    "in_df.show()\n",
    "from pyspark.sql.functions import posexplode_outer, split\n",
    "\n",
    "in_df.withColumn(\"Qualification\",\n",
    "explode_outer(split(\"Education\",\",\"))).show()\n",
    "in_df.select(\"*\",posexplode_outer(split(\"Education\",\",\")))\\\n",
    "     .withColumnRenamed(\"col\", \"Qualification\")\\\n",
    "     .withColumnRenamed (\"pos\",\"Index\")\\\n",
    "     .drop(“Education”).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362abf13-a173-4315-9e74-c7a675b08185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4749fc-c9cd-481a-8222-809e419f22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is SparkConf in PySpark? List a few attributes of SparkConf.\n",
    "SparkConf aids in the setup and settings needed to execute a spark application locally or in a cluster. To put it another way, it\n",
    "offers settings for running a Spark application. The following are some of SparkConf's most important features:\n",
    "• set(key, value): This attribute aids in the configuration property setting.\n",
    "• setSparkHome(value): This feature allows you to specify the directory where Spark will be installed on worker nodes.\n",
    "• setAppName(value): This element is used to specify the name of the application.\n",
    "• setMaster(value): The master URL may be set using this property.\n",
    "• get(key, defaultValue=None): This attribute aids in the retrieval of a key's configuration value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c4c5e7-a9e4-4846-8957-131a42416aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Create an RDD\n",
    "# rdd = spark.sparkContext.parallelize(Seq((\"A\",1),(\"A\",3),(\"B\",4),(\"B\",2),(\"C\",5)))  \n",
    "# // Get the data in RDD\n",
    "# rdd_collect = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca5b35f-c310-411e-a1bf-e40753134d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 11:06:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before repartitioning: 4\n",
      "Number of partitions after repartitioning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (2 + 2) / 4]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RepartitionExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22), (\"David\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Check the number of partitions before repartitioning\n",
    "print(\"Number of partitions before repartitioning:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Repartition the DataFrame into 2 partitions\n",
    "df_repartitioned = df.repartition(2)\n",
    "\n",
    "# Check the number of partitions after repartitioning\n",
    "print(\"Number of partitions after repartitioning:\", df_repartitioned.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ae71e3-ebe2-456a-8bc0-9512d415dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before coalescing: 4\n",
      "Number of partitions after coalescing: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 11:12:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CoalesceExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22), (\"David\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Check the number of partitions before coalescing\n",
    "print(\"Number of partitions before coalescing:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce the DataFrame into 2 partitions\n",
    "df_coalesced = df.coalesce(2)\n",
    "\n",
    "# Check the number of partitions after coalescing\n",
    "print(\"Number of partitions after coalescing:\", df_coalesced.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05babe82-2767-4774-a441-b0657e0d0790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 15:55:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"GlobalSparkSession2\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a9ab82fd-a10b-4b5b-8bf3-c2f61c0b6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| Name|eng|math|\n",
      "+-----+---+----+\n",
      "|Shivu| 59|  68|\n",
      "|Rudra| 60|  79|\n",
      "|  Anu| 80|  65|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Transform the input DataFrame into a new DataFrame , where each row represents a unique student,\n",
    "and the columns include the student's name along with the marks for the \"math\" and \"eng\" subjects.\n",
    "\"\"\"\n",
    "\n",
    "data=[\n",
    "('Rudra','math',79),\n",
    "('Rudra','eng',60),\n",
    "('Shivu','math', 68),\n",
    "('Shivu','eng', 59),\n",
    "('Anu','math', 65),\n",
    "('Anu','eng',80)\n",
    "]\n",
    "schema=\"Name string,Sub string,Marks int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.groupBy(\"Name\").pivot(\"Sub\").max(\"Marks\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a00b7e25-e340-410d-8bf2-00e2edb8532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task - Find the missing numbers in the column\n",
    "\n",
    "data = [(1, ),(2,),(3,),(6,),(7,),(8,)]\n",
    "df = spark.createDataFrame(data,['id'])#.toDF(\"id\")\n",
    "nums=spark.range(1,9)\n",
    "nums.join(df,nums.id==df.id,'left').select(nums.id).filter(df.id.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e38200-5d33-45dc-99b4-7776decf76e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+---------+\n",
      "|ticket_id|opened_date|closed_date|diff_days|\n",
      "+---------+-----------+-----------+---------+\n",
      "|       t1| 2023-01-01| 2023-01-10|       10|\n",
      "|       t2| 2023-01-01| 2023-01-14|       14|\n",
      "+---------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Asked in synechron \n",
    "return the gap between opened and closed date of a ticket\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import * #to_date,col\n",
    "\n",
    "data = [(\"t1\", \"01/01/2023\", \"opened\"),\n",
    "        (\"t1\", \"05/01/2023\", \"assigned\"),\n",
    "        (\"t1\", \"07/01/2023\", \"under_review\"),\n",
    "        (\"t1\", \"10/01/2023\", \"closed\"),\n",
    "        (\"t2\", \"01/01/2023\", \"opened\"),\n",
    "        (\"t2\", \"05/01/2023\", \"assigned\"),\n",
    "        (\"t2\", \"07/01/2023\", \"under_review\"),\n",
    "        (\"t2\", \"10/01/2023\", \"under_manager_review\"),\n",
    "        (\"t2\", \"14/01/2023\", \"closed\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"ticket_id\", \"date_changed\", \"status\"])\n",
    "df=df.withColumn(\"date_changed\",to_date(col(\"date_changed\"),'dd/MM/yyyy'))#.show()\n",
    "# df.printSchema()\n",
    "df.groupBy(\"ticket_id\").agg(min(\"date_changed\").alias(\"opened_date\"),\\\n",
    "                        max(\"date_changed\").alias(\"closed_date\"))\\\n",
    ".withColumn(\"diff_days\",datediff(\"closed_date\",\"opened_date\")+1)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4bfe3-340c-4eb1-bb8e-9374102f669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What are SparkFiles in Pyspark?\n",
    "PySpark provides the reliability needed to upload our files to Apache Spark. This is accomplished by using sc.addFile,\n",
    "where 'sc' stands for SparkContext. We use SparkFiles.net to acquire the directory path.\n",
    "We use the following methods in SparkFiles to resolve the path to the files added using SparkContext.addFile():\n",
    "• get(filename),\n",
    "• getrootdirectory()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ca6021-5426-4d1a-a79d-fd219af7d986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: bigint, interaction_type: string, click_count: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Find the top 5 customers with the highest number of clicks.\n",
    "The click dataset has columns like 'user_id', 'timestamp', 'interaction_type'.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import count\n",
    "data_tuples = [\n",
    "(1, '2023-01-01 12:00:00', 'click'),\n",
    "(2, '2023-01-01 12:05:00', 'view'),\n",
    "(1, '2023-01-01 12:10:00', 'click'),\n",
    "(3, '2023-01-01 12:15:00', 'view'),\n",
    "(2, '2023-01-01 12:20:00', 'click'),\n",
    "(1, '2023-01-01 12:25:00', 'view'),\n",
    "(3, '2023-01-01 12:30:00', 'click'),\n",
    "(2, '2023-01-01 12:35:00', 'view'),\n",
    "(1, '2023-01-01 12:40:00', 'click'),\n",
    "(3, '2023-01-01 12:45:00', 'view'),\n",
    "(1, '2023-01-02 12:10:00', 'click'),\n",
    "(1, '2023-01-03 12:10:00', 'click'),\n",
    "(1, '2023-01-04 12:10:00', 'view'),(3, '2023-01-01 12:40:00', 'click'),\n",
    "(3, '2023-01-01 12:45:00', 'view'),\n",
    "# Add more tuples as needed\n",
    "]\n",
    "schema=['user_id', 'timestamp', 'interaction_type']\n",
    "df=spark.createDataFrame(data_tuples,schema)\n",
    "df.groupBy(\"user_id\",\"interaction_type\").agg(count(\"interaction_type\").alias(\"click_count\"))\\\n",
    "    .where(\"interaction_type='click'\")\\\n",
    "    .orderBy(\"click_count\",ascending=False)\\\n",
    "    .limit(5)#\\.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc14147-3427-4a74-b1c2-1341fc8a9fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Type: string, Administration: double, Management: double, Technical: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A PivotTable is especially designed for querying large amounts of data in many user-friendly ways to derive quick insights easily.\n",
    "Task - Show work group wise , average salary for each department\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "        'First Name': ['Aryan', 'Rohan', 'Riya', 'Yash', 'Siddhant'],\n",
    "        'Last Name': ['Singh', 'Agarwal', 'Shah', 'Bhatia', 'Khanna'],\n",
    "        'Type': ['Full-time Employee', 'Intern', 'Full-time Employee', 'Part-time Employee', 'Full-time Employee'],\n",
    "        'Department': ['Administration', 'Technical', 'Administration', 'Technical', 'Management'],\n",
    "        'YoE': [2, 3, 5, 7, 6],\n",
    "        'Salary': [20000, 5000, 10000, 10000, 20000]\n",
    "       }\n",
    "import pandas as pd\n",
    "pd_table=pd.DataFrame(data)\n",
    "df=spark.createDataFrame(pd_table)\n",
    "df.groupBy('Type').pivot('Department').avg('Salary').na.fill(0)\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b506c9f-4282-4c14-9133-c007789d2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 'camelCase to snake_case'\n",
    "Task - Change the pyspark dataframe column names from camel case to snake case.\n",
    "\"\"\"\n",
    "schema=[\"userId\",\"firstName\",\"last_name\"]\n",
    "data = [\n",
    "(101 , 'Neha' , 'K'),\n",
    "(102 , 'Mark' , 'T'),\n",
    "(103 , 'Iram' , 'D')\n",
    "]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "fields=df.columns\n",
    "def convert_to_snake_case(charArr):\n",
    "    return ''.join(['_'+char.lower() if char.isupper() else char  for char in charArr])\n",
    "# snake_columns=list(map(convert_to_snake_case,fields))\n",
    "snake_columns=[convert_to_snake_case(s) for s in fields]\n",
    "df=spark.createDataFrame(data,snake_columns)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3ed5661-56a0-4334-8323-99665829a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the price difference between the months in the same year.\n",
    "Pending due to inconsistend data\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import datediff,to_date,col,year,lag\n",
    "from pyspark.sql.window import Window\n",
    "data = [\n",
    "    (1,'2023-01-01',10),(1,'2023-01-27',13),(1,'2023-02-01',14)\n",
    "    ,(1,'2023-02-15',15),(1,'2023-03-03',18),(1,'2023-03-27',15)\n",
    "    ,(1,'2023-04-06',20),(2,'2024-01-01',50),(2,'2024-01-29',100)\n",
    "    ,(2,'2024-02-01',150)\n",
    "]\n",
    "schema = \"sku_id int , price_date string , price int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "windowSpec=Window.partitionBy(col(\"sku_id\")).orderBy(col(\"price\"))\n",
    "getPrice=df.withColumn(\"month_year\",col(\"price_date\")[0:7])\\\n",
    ".groupBy(\"month_year\").sum(\"price\")\\\n",
    "# .show()\n",
    "   # .withColumn(\"prev_price\",lag(\"price\").over(windowSpec))\n",
    "# getPrice.withColumn(\"price_diff\",col(\"price\")-col(\"prev_price\"))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a88ef30-ddce-4b64-8717-4fc88091b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "🚀 Your Task:\n",
    "Implement a PYSPARK program to solve these challenges. Share your code in the comments below and let the coding battle begin! 💻🔥\n",
    " \n",
    "1️⃣ Get only Year part of \"JoiningDate\"\n",
    "2️⃣ Get only Month part of \"JoiningDate\".\n",
    "3️⃣ Get only date part of \"JoiningDate\".\n",
    "4️⃣Get the current system date using DataFrame API\n",
    "5️⃣Get the current UTC date and time using DataFrame API\n",
    "\"\"\"\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date,col,to_timestamp,year,month,day,hour,minute,second,current_date,to_utc_timestamp\n",
    "\n",
    "data = [\n",
    "   [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"],\n",
    "   [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"],\n",
    "   [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"],\n",
    "   [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"],\n",
    "   [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"],\n",
    "]\n",
    " \n",
    "# Create a schema for the DataFrame\n",
    "schema = StructType([\n",
    "   StructField(\"EmployeeID\", IntegerType(), True),\n",
    "   StructField(\"First_Name\", StringType(), True),\n",
    "   StructField(\"Last_Name\", StringType(), True),\n",
    "   StructField(\"Salary\", DoubleType(), True),\n",
    "   StructField(\"Joining_Date\", StringType(), True),\n",
    "   StructField(\"Department\", StringType(), True),\n",
    "   StructField(\"Gender\", StringType(), True)\n",
    "])\n",
    "  \n",
    "df=spark.createDataFrame(data,schema)\n",
    "clean_data=df.withColumn(\"Joining_Date\",col(\"Joining_Date\")[0:19])\\\n",
    ".withColumn(\"Joining_Date\",to_timestamp(\"Joining_Date\",'yyyy-MM-dd HH:mm:ss'))\\\n",
    ".withColumn(\"Year\",year(\"Joining_Date\"))\\\n",
    ".withColumn(\"Month\",month(\"Joining_Date\"))\\\n",
    ".withColumn(\"Day\",day(\"Joining_Date\"))\\\n",
    ".withColumn(\"SystemDate\",current_date())\\\n",
    ".withColumn(\"UTCTime\",to_utc_timestamp(current_date(),\"PST\"))\\\n",
    "# .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa250f4b-0e0d-429b-aa28-64b8942b9676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, length: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenge : 𝐅𝐢𝐧𝐝 𝐭𝐡𝐞 𝐭𝐨𝐭𝐚𝐥 𝐜𝐨𝐧𝐬𝐨𝐧𝐚𝐧𝐭𝐬 𝐜𝐨𝐮𝐧𝐭 𝐨𝐟 𝐭𝐡𝐞 𝐦𝐞𝐬𝐬𝐚𝐠𝐞 𝐜𝐨𝐥𝐮𝐦𝐧.\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import ArrayType,StringType\n",
    "data = [(1,'I love to play cricket',),(2,'I am into motorbiking',),(3,'What do you like',)]\n",
    "schema = [\"id\", \"message\"]\n",
    "df = spark.createDataFrame(data = data , schema = schema)\n",
    "# vowels=['a','e','i','o','u']\n",
    "# return the consonant string\n",
    "def find_consonants(str):\n",
    "    cons=[]\n",
    "    for char in str.replace(\" \",\"\"):\n",
    "# use isalpha() method to filter out no-alphabetic characters\n",
    "        if char.isalpha() and char.lower() not in ['a','e','i','o','u']:\n",
    "            cons.append(char)\n",
    "    return ''.join(cons),len(cons)\n",
    "# register udf\n",
    "consonant_udf=udf(find_consonants,returnType=ArrayType(StringType()))\n",
    "apply_udf=df.withColumn(\"consonant\",consonant_udf(col(\"message\")))\n",
    "apply_udf.select(col(\"id\")\n",
    "                 ,col(\"consonant\")[0].alias(\"text\")\n",
    "                 ,col(\"consonant\")[1].alias(\"length\")\n",
    "                )\\\n",
    "# .show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c2b6a31-e1b9-4b97-99d6-1f019ca838b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer: string, st_start: string, st_end: string, ed_start: string, ed_end: string]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Challenge : 𝑭𝒊𝒏𝒅 𝒕𝒉𝒆 𝒔𝒕𝒂𝒓𝒕 𝒂𝒏𝒅 𝒆𝒏𝒅 𝒍𝒐𝒄𝒂𝒕𝒊𝒐𝒏 𝒇𝒐𝒓 𝒆𝒂𝒄𝒉 𝒄𝒖𝒔𝒕𝒐𝒎𝒆𝒓.\n",
    "df.show()\n",
    "df.groupBy(\"customer\")\\\n",
    ".agg(collect_list(\"start_location\").alias(\"start_locations\"),\n",
    "     collect_list(\"end_location\").alias(\"end_locations\"))\\\n",
    ".show(truncate=False)\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "data = [ ('c1', 'New York', 'Lima'),('c1', 'London', 'New York'),\n",
    "('c1', 'Lima', 'Sao Paulo'),('c1', 'Sao Paulo', 'New Delhi'),\n",
    "('c2', 'Mumbai', 'Hyderabad'),('c2', 'Surat', 'Pune'),\n",
    "('c2', 'Hyderabad', 'Surat'),('c3', 'Kochi', 'Kurnool'),\n",
    "('c3', 'Lucknow', 'Agra'),('c3', 'Agra', 'Jaipur'),('c3', 'Jaipur', 'Kochi')]\n",
    "\n",
    "schema = \"customer string , start_location string , end_location string\"\n",
    "df = spark.createDataFrame(data = data , schema = schema).createOrReplaceTempView(\"journeys\")\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "st.customer\n",
    ",st.start_location as st_start,st.end_location as st_end\n",
    ",ed.start_location as ed_start,ed.end_location as ed_end\n",
    "\n",
    "from journeys as st\n",
    "left join journeys as ed\n",
    "on st.start_location=ed.end_location\n",
    "order by st.customer\n",
    "\"\"\"\n",
    ")\\\n",
    "# .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5ee6140c-a79b-41f8-89b6-b66ba6c42d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Challenge: 𝐅𝐢𝐧𝐝 𝐫𝐞𝐜𝐨𝐫𝐝𝐬 𝐢𝐧 𝐚 𝐝𝐚𝐭𝐚𝐟𝐫𝐚𝐦𝐞 𝐰𝐡𝐢𝐜𝐡 𝐚𝐫𝐞 𝐧𝐨𝐭 𝐩𝐫𝐞𝐬𝐞𝐧𝐭 𝐢𝐧 𝐚𝐧𝐨𝐭𝐡𝐞𝐫 𝐝𝐚𝐭𝐚𝐟𝐫𝐚𝐦𝐞.\n",
    "\"\"\"\n",
    "data_1 = [\n",
    "Row(id=1, name='Mumbai'),\n",
    "Row(id=2, name='Bangalore'),\n",
    "Row(id=3, name='Delhi')\n",
    "]\n",
    "\n",
    "data_2 = [\n",
    "Row(id=2, name='Bangalore'),\n",
    "Row(id=1, name='Mumbai'),\n",
    "Row(id=4, name='Ayodhya')\n",
    "]\n",
    "\n",
    "# Creating DataFrame 1\n",
    "df_1 = spark.createDataFrame(data_1)\n",
    "\n",
    "# Creating DataFrame 2\n",
    "df_2 = spark.createDataFrame(data_2)\n",
    "df_1.exceptAll(df_2)#.show()\n",
    "# df_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f0899fa-0f6b-4c8f-a74c-ef344296b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Challenge: How to unpivot a dataframe in PySpark. Explain me with the help of example.\n",
    "Miscellaneous: What if we have 100+ columns in the dataframe.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import lit,when\n",
    "\n",
    "cricket_data = [\n",
    "    (\"Virat Kohli\", 85, 100, 75),(\"Steve Smith\", 90, 105, 80),(\"Kane Williamson\", 88, 95, 70)\n",
    "               ]\n",
    "\n",
    "row_df=spark.createDataFrame([(\"Match1\",),(\"Match2\",),(\"Match3\",)],[\"Matches\"])\n",
    "\n",
    "df = spark.createDataFrame(cricket_data, [\"Player\", \"Match1\", \"Match2\", \"Match3\"])\n",
    "df2=df.crossJoin(row_df)\\\n",
    ".withColumn(\"Scores\"\n",
    "            ,when(col('Matches')==\"Match1\",col(\"Match1\"))\n",
    "            .when(col('Matches')==\"Match2\",col(\"Match2\"))\n",
    "            .when(col('Matches')==\"Match3\",col(\"Match3\"))\n",
    "           ).select(\"Player\",\"Matches\",\"Scores\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "457408b4-7d2d-447f-93b4-2b4160161c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dep_id: int, min_sal_emp: string, max_sal_emp: string]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "𝑪𝒉𝒂𝒍𝒍𝒆𝒏𝒈𝒆 : Get the name of the 𝐡𝐢𝐠𝐡𝐞𝐬𝐭 𝐬𝐚𝐥𝐚𝐫𝐲 𝐚𝐧𝐝 𝐥𝐨𝐰𝐞𝐬𝐭 𝐬𝐚𝐥𝐚𝐫𝐲 employee name in 𝐞𝐚𝐜𝐡 𝐝𝐞𝐩𝐚𝐫𝐭𝐦𝐞𝐧𝐭.\n",
    "If the salary is same then return the name of the employee whose name comes first in 𝐥𝐞𝐱𝐢𝐜𝐨𝐠𝐫𝐚𝐩𝐡𝐢𝐜𝐚𝐥 𝐨𝐫𝐝𝐞𝐫.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "emp_data = [ ('Siva',1,30000),('Ravi',2,40000),('Prasad',1,50000),('Arun',1,30000),('Sai',2,20000) ]\n",
    "\n",
    "emp_schema = \"emp_name string , dep_id int , salary long\"\n",
    "\n",
    "# Creating the dataframe\n",
    "df = spark.createDataFrame(data = emp_data , schema = emp_schema)\n",
    "\n",
    "windowSpec_min=Window.partitionBy(\"dep_id\").orderBy(\"salary\",\"emp_name\")\n",
    "windowSpec_max=Window.partitionBy(\"dep_id\").orderBy(col(\"salary\").desc(),\"emp_name\")\n",
    "\n",
    "rank_salaries=df.withColumn(\"min_sal_rank\",rank().over(windowSpec_min))\\\n",
    "                .withColumn(\"max_sal_rank\",rank().over(windowSpec_max))\\\n",
    "                .selectExpr(\"dep_id\",\"case when min_sal_rank=1 then emp_name end as min_sal_emp\"\\\n",
    "                          ,\"case when max_sal_rank=1 then emp_name end as max_sal_emp\")\\\n",
    "\n",
    "rank_salaries.groupBy(\"dep_id\").agg(max(\"min_sal_emp\").alias(\"min_sal_emp\")\\\n",
    "                    ,max(\"max_sal_emp\").alias(\"max_sal_emp\"))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b47549-f46f-496d-a46d-b3a418d16564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Challenge : Find the origin and the destination of each customer.\n",
    "Note : There can be more than 1 stop for the same customer journey.\n",
    "\n",
    "join(df_flight,(df_flight.cust_id==get_flightIds.cust_id) &\n",
    "                  ((get_flightIds.min_flight==df_flight.flight_id) | (get_flightIds.max_flight==df_flight.flight_id))\n",
    "                  )\\\n",
    ".agg(min(\"flight_id\").alias(\"min_flight\"),\n",
    "                                max(\"flight_id\").alias(\"max_flight\"))\n",
    "\"\"\"\n",
    "flights_data = [(1,'Flight1' , 'Delhi' , 'Hyderabad'),\n",
    "(1,'Flight2' , 'Hyderabad' , 'Kochi'),(1,'Flight3' , 'Kochi' , 'Mangalore'),\n",
    "(2,'Flight1' , 'Mumbai' , 'Ayodhya'),(2,'Flight2' , 'Ayodhya' , 'Gorakhpur')\n",
    "]\n",
    "\n",
    "_schema = \"cust_id int, flight_id string , origin string , destination string\"\n",
    "df_flight = spark.createDataFrame(data = flights_data , schema= _schema)\n",
    "# df_flight.show()\n",
    "\n",
    "# get_flightIds=df_flight.groupBy(\"cust_id\")\\\n",
    "# .withColumn(\"origin_1\",when(col(\"flight_id\")==min(\"flight_id\"),col(\"origin\")))\n",
    "\n",
    "# get_flightIds\\\n",
    "# # .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c665fee4-9256-42af-b64b-caf26ba877b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+-------+\n",
      "|name|age|mono_id|n_parts|\n",
      "+----+---+-------+-------+\n",
      "+----+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Challenge:- Read the third quarter (25%) of emp_data table.\n",
    "\"\"\"\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile,col,monotonically_increasing_id\n",
    "emp_data = [(\"Alice\", 28),\n",
    "(\"Bob\", 35),(\"Harvey\",45),(\"Donna\",45),\n",
    "(\"Charlie\", 42),\n",
    "(\"David\", 25),\n",
    "(\"Eva\", 31),\n",
    "(\"Frank\", 38),\n",
    "(\"Grace\", 45),\n",
    "(\"Henry\", 29)]\n",
    "\n",
    "emp_schema = \"name string , age int\"\n",
    "\n",
    "df = spark.createDataFrame(data = emp_data , schema = emp_schema)\n",
    "createId=df.withColumn(\"mono_id\",monotonically_increasing_id())\n",
    "createId.withColumn(\"n_parts\",ntile(4).over(Window.partitionBy(\"mono_id\").orderBy(\"name\"))).where(col(\"n_parts\")==3)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc39f94-78cb-436f-a2b0-81625f10fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+\n",
      "|emp_id|  log_date|isLogged_in|\n",
      "+------+----------+-----------+\n",
      "|   101|2024-01-03|          Y|\n",
      "|   101|2024-01-07|          Y|\n",
      "|   102|2024-01-02|          Y|\n",
      "|   102|2024-01-03|          Y|\n",
      "|   102|2024-01-05|          Y|\n",
      "|   102|2024-01-06|          Y|\n",
      "|   102|2024-01-07|          Y|\n",
      "|   103|2024-01-05|          Y|\n",
      "|   103|2024-01-06|          Y|\n",
      "+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task : Retrieve information about consecutive login streaks for employees who have logged in for at least two consecutive days.\n",
    "\n",
    "For each employee, provide the emp_id , the number of consecutive days logged in \n",
    ",the start_date of the streak and end_date of the streak.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col,to_date,lag,date_diff\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "_data = [(101,'02-01-2024','N'),(101,'03-01-2024','Y'),(101,'04-01-2024','N'),\n",
    "(101,'07-01-2024','Y'),(102,'01-01-2024','N'),(102,'02-01-2024','Y'),(102,'03-01-2024','Y'),\n",
    "(102,'04-01-2024','N'),(102,'05-01-2024','Y'),(102,'06-01-2024','Y'),(102,'07-01-2024','Y'),\n",
    "(103,'01-01-2024','N'),(103,'04-01-2024','N'),(103,'05-01-2024','Y'),(103,'06-01-2024','Y'),(103,'07-01-2024','N')\n",
    "]\n",
    "_schema = [\"emp_id\" , \"log_date\" , \"isLogged_in\"]\n",
    "\n",
    "# creating the dataframe\n",
    "df = spark.createDataFrame(data = _data , schema = _schema)\n",
    "filter_Yes=df.withColumn(\"log_date\",to_date(\"log_date\",\"dd-MM-yyyy\")).where(col(\"isLogged_in\")=='Y').show()\n",
    "windowSpec=Window.partitionBy(\"emp_id\").orderBy(\"log_date\")\n",
    "# calc_diff=filter_Yes.withColumn(\"diff_days\",date_diff(col(\"log_date\"),lag(\"log_date\").over(windowSpec)))\n",
    "# consecutive_emp=calc_diff.where(col(\"diff_days\")==1).distinct()\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "510c0f5f-2b1c-41e7-a4a4-55c9b0bae359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Course_type: string, Head_Office_Contact: bigint, Institute_Name: string, State: string, City: string, address: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the given Json_data:\n",
    "companies={\n",
    "        \"Institute_Name\" : \"ABC_Coaching_Center\",\n",
    "        \"Course_type\" : \"Best_seller\" ,\n",
    "        \"branches\" : [\n",
    "        {\n",
    "        \"State\" : \"Maharashtra\",\n",
    "        \"City\" : \"Mumbai\",\n",
    "        \"address\" : \"XYZ\"\n",
    "        },\n",
    "        {\n",
    "        \"State\" : \"Gujrat\",\n",
    "        \"City\" : \"Surat\",\n",
    "        \"address\" : \"PQRX\"\n",
    "        }\n",
    "        ],\n",
    "        \"Head_Office_Contact\" : 8787878787\n",
    "        }\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_pd=pd.DataFrame(companies)\n",
    "df=spark.createDataFrame(df_pd)\n",
    "df.select(\"Course_type\",\"Head_Office_Contact\",\"Institute_Name\",col(\"branches\")[\"State\"].alias(\"State\")\n",
    "          ,col(\"branches\")[\"City\"].alias(\"City\")\n",
    "          ,col(\"branches\")[\"address\"].alias(\"address\"))\\\n",
    "# .show(truncate=False)\n",
    "# print(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2768d327-af93-4e18-8e04-9de0a5cd1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+------------------+---------------------+-------------+\n",
      "|product_id|product_name|category|years             |total_sales          |increase_flag|\n",
      "+----------+------------+--------+------------------+---------------------+-------------+\n",
      "|2         |Jeans       |Clothing|[2019, 2020, 2021]|[500.0, 600.0, 900.0]|Y            |\n",
      "+----------+------------+--------+------------------+---------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task : Write a sql query to find the products whose total sales revenue has increased every year.\n",
    "Include the product_id, product_name and category in the result.\n",
    "\"\"\"\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,when,collect_list,udf\n",
    "\n",
    "product_data = [(1, 'Laptops', 'Electronics'),(2, 'Jeans', 'Clothing'),(3, 'Chairs', 'Home Appliances')]\n",
    "\n",
    "sales_data = [\n",
    "(1, 2019, 1000.00),\n",
    "(1, 2020, 1200.00),\n",
    "(1, 2021, 1100.00),\n",
    "(2, 2019, 500.00),\n",
    "(2, 2020, 600.00),\n",
    "(2, 2021, 900.00),\n",
    "(3, 2019, 300.00),\n",
    "(3, 2020, 450.00),\n",
    "(3, 2021, 400.00)\n",
    "]\n",
    "\n",
    "product_schema = ['product_id', 'product_name', 'category']\n",
    "sales_schema = ['product_id', 'year', 'total_sales_revenue']\n",
    "\n",
    "products= spark.createDataFrame(data = product_data , schema = product_schema)\n",
    "sales = spark.createDataFrame(data = sales_data , schema = sales_schema)\n",
    "\n",
    "joined_data=products.join(sales,products.product_id==sales.product_id)\\\n",
    "        .select(products.product_id,\"product_name\",\"category\",\"year\",\"total_sales_revenue\")\n",
    "collect_data=joined_data.groupBy(\"product_id\",\"product_name\",\"category\")\\\n",
    ".agg(collect_list(\"year\").alias(\"years\"),collect_list(\"total_sales_revenue\").alias(\"total_sales\"))\\\n",
    "\n",
    "def find_flag(lst):\n",
    "    flag=\"Y\"\n",
    "    for i in range(len(lst)-1):\n",
    "        if(lst[i+1]<lst[i]):\n",
    "            flag=\"N\"\n",
    "            return\n",
    "    return flag\n",
    "\n",
    "increase_udf=udf(find_flag)\n",
    "collect_data.withColumn(\"increase_flag\",increase_udf(col(\"total_sales\"))).filter(col(\"increase_flag\")=='Y')\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b5a1ad15-c98d-4293-af9a-1ac3f5f1d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 154:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+------+------+-------+-----------+\n",
      "|emp_id|emp_name| emp_role|dept_id|emp_id|salary|dept_id|  dept_name|\n",
      "+------+--------+---------+-------+------+------+-------+-----------+\n",
      "|     1|    John|  Manager|      1|     1|100000|      1|Engineering|\n",
      "|     3|     Bob|Developer|      3|     3| 95000|      3|      Sales|\n",
      "|     2|   Alice|Developer|      2|     2| 90000|      2|  Marketing|\n",
      "+------+--------+---------+-------+------+------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How to join more than two dataframes\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example DataFrame 1: Employee data\n",
    "employee_data = [\n",
    "    (1, \"John\", \"Manager\",1),\n",
    "    (2, \"Alice\", \"Developer\",2),\n",
    "    (3, \"Bob\", \"Developer\",3)\n",
    "]\n",
    "employee_schema = [\"emp_id\", \"emp_name\", \"emp_role\",\"dept_id\"]\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "# Example DataFrame 2: Department data\n",
    "department_data = [\n",
    "    (1, \"Engineering\"),\n",
    "    (2, \"Marketing\"),\n",
    "    (3, \"Sales\")\n",
    "]\n",
    "department_schema = [\"dept_id\", \"dept_name\"]\n",
    "department_df = spark.createDataFrame(department_data, schema=department_schema)\n",
    "\n",
    "# Example DataFrame 3: Salary data\n",
    "salary_data = [\n",
    "    (1, 100000),\n",
    "    (2, 90000),\n",
    "    (3, 95000)\n",
    "]\n",
    "salary_schema = [\"emp_id\", \"salary\"]\n",
    "salary_df = spark.createDataFrame(salary_data, schema=salary_schema)\n",
    "\n",
    "# Perform Join/\n",
    "join_data=(\n",
    "    employee_df.join(salary_df,employee_df.emp_id==salary_df.emp_id)\\\n",
    "    .join(department_df,employee_df.dept_id==department_df.dept_id,\"left\")\n",
    ").show()\n",
    "                 # department_df,employee_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aaa7b4a6-134e-45cd-b742-338e95eacc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem: You have a dataset of sales transactions, and you want to find the top-selling product for each month.\n",
    "Write a PySpark code to achieve this using window functions.\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType,StructField,StringType,DateType,DoubleType\n",
    "import pyspark.sql.functions  as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"sales_date\", DateType(), True),\n",
    "    StructField(\"sales_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"A\", datetime.strptime(\"2023-01-15\", \"%Y-%m-%d\").date(), 100.0),\n",
    "    (\"B\", datetime.strptime(\"2023-01-20\", \"%Y-%m-%d\").date(), 150.0),\n",
    "    (\"A\", datetime.strptime(\"2023-02-10\", \"%Y-%m-%d\").date(), 120.0),\n",
    "    (\"B\", datetime.strptime(\"2023-02-15\", \"%Y-%m-%d\").date(), 180.0),\n",
    "    (\"C\", datetime.strptime(\"2023-03-05\", \"%Y-%m-%d\").date(), 200.0),\n",
    "    (\"A\", datetime.strptime(\"2023-03-10\", \"%Y-%m-%d\").date(), 250.0)\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "windowSpec=Window.partitionBy(F.year(\"sales_date\"),F.month(\"sales_date\")).orderBy(F.col(\"sales_amount\").desc())\n",
    "get_max_price=df.withColumn(\"year\",F.year(\"sales_date\")).withColumn(\"month\",F.month(\"sales_date\"))\\\n",
    "                .withColumn(\"price_rank\",F.rank().over(windowSpec)).filter(col(\"price_rank\")==1)\\\n",
    "# .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65eca84d-8e8f-4f38-8886-7fda31283e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncount_empid=employee_df.groupBy(\"employee_id\").agg(F.count(F.col(\"employee_id\")).alias(\"count_emp\"))\\ndecide_emp=employee_df.join(count_empid,employee_df.employee_id==count_empid.employee_id).withColumn(\"flag\",F.expr(\"CASE WHEN count_emp >1 and primary_flag =\\'Y\\' then \\'Y\\' else \\'N\\' end\")).where(F.expr(\"count_emp=1 or flag=\\'Y\\'\")).show()\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write a Pyspark code to find primary Department for Each Employee\n",
    "Employees can belong to multiple departments. When the employee joins other departments,\n",
    "they need to decide which department is their primary department. Note that when an employee belongs to only one department, \n",
    "their primary column is 'N'.\n",
    "Write a solution to report all the employees with their primary department. For employees who belong to one department,\n",
    "report their only department.\n",
    "Return the result in any order.\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "import pyspark.sql.functions  as F\n",
    "# Define the schema for the Employee table\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"primary_flag\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Employee DataFrame\n",
    "employee_data = [  (1, 1, \"N\"),  (2, 1, \"Y\"), (2, 2, \"N\"), (3, 3, \"N\"), (4, 2, \"N\"), (4, 3, \"Y\"), (4, 4, \"N\") ]\n",
    "\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\"\"\"\n",
    "count_empid=employee_df.groupBy(\"employee_id\").agg(F.count(F.col(\"employee_id\")).alias(\"count_emp\"))\n",
    "decide_emp=employee_df.join(count_empid,employee_df.employee_id==count_empid.employee_id)\\\n",
    ".withColumn(\"flag\",F.expr(\"CASE WHEN count_emp >1 and primary_flag ='Y' then 'Y' else 'N' end\"))\\\n",
    ".where(F.expr(\"count_emp=1 or flag='Y'\"))\\\n",
    ".show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "23794833-2294-46c6-9b56-331f52c9e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Find City name that starts with vowels [A,I,O,U,E] 🤔.\n",
    "City names should not repeat in the result dataset using pyspark.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Lat_N\", DoubleType(), True),\n",
    "    StructField(\"Long_W\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 'New York', 'NY', 40.7128, -74.006),\n",
    "    (2, 'Los Angeles', 'CA', 34.0522, -118.2437),\n",
    "    (3, 'Albany', 'IL', 41.8781, -87.6298),\n",
    "    (4, 'East China', 'TX', 29.7604, -95.3698),\n",
    "    (5, 'East China', 'AZ', 33.4484, -112.074),\n",
    "    (6, 'Philadelphia', 'PA', 39.9526, -75.1652),\n",
    "    (7, 'San Antonio', 'TX', 29.4241, -98.4936),\n",
    "    (8, 'San Diego', 'CA', 32.7157, -117.1611),\n",
    "    (9, 'Oakfield', 'TX', 32.7767, -96.797),\n",
    "    (10, 'San Jose', 'CA', 37.3382, -121.8863)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "# Filter city names that start with vowels\n",
    "vowel_cities= df.filter(col(\"City\").rlike('^[AEIOUaeiou]'))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fb0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/29 22:03:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"GlobalSparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3828da9e-d290-4b3f-89a9-3003c6adce91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, count: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given an input pyspark dataframe, columns as account_id and income.\n",
    "Write a solution to calculate the number of bank accounts for each salary category. The salary categories are:\n",
    "\"Low Salary\": All the salaries are strictly less than $20000.\n",
    "\"Average Salary\": All the salaries in the inclusive range [$20000, $50000].\n",
    "\"High Salary\": All the salaries strictly greater than $50000.\n",
    "The result table must contain all three categories. If there are no accounts in a category, return 0.\n",
    "Return the result table in any order.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"income\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Insert data into the DataFrame\n",
    "data = [ (3, 108939),   (2, 12747), (8, 87709), (6, 91796) ]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Define the CTE with the income category logic.\n",
    "\n",
    "cte = df.withColumn(\"category\",\n",
    "     when(col(\"income\") < 20000, \"Low Salary\")\n",
    "    .when((col(\"income\") >= 20000) & (col(\"income\") <= 50000), \"Average Salary\")\n",
    "    .when(col(\"income\") > 50000, \"High Salary\")\n",
    "    .otherwise(\"\")\n",
    ")\n",
    "\n",
    "# Define the 'salary' DataFrame with distinct categories\n",
    "salary = spark.createDataFrame([(\"Low Salary\",), (\"Average Salary\",), (\"High Salary\",)], [\"category\"])\n",
    "cte.join(salary,salary.category==cte.category,\"right\")\\\n",
    ".groupBy(salary.category).agg(count(cte.category).alias(\"count\"))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2944ab7f-1bf5-44dc-a2cc-b258015866d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[player: int, player_name: string, p_count: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Leet code 1783. Grand slam titles\n",
    "Find how many titles did a player won\n",
    "\"\"\"\n",
    "players_data = [(1, 'Nadal'), (2, 'Federer'), (3, 'Novak')]\n",
    "championships_data = [(2018, 1, 1, 1, 1), (2019, 1, 1, 2, 2), (2020, 2, 1, 2, 2)]\n",
    "players_schema=['player_id','player_name']\n",
    "champions_schema='year int,wimbledon int,Fr_open int,US_open int,AUopen int'\n",
    "players_df = spark.createDataFrame(players_data, schema=players_schema)\n",
    "champions_df = spark.createDataFrame(championships_data, schema=champions_schema).createOrReplaceTempView(\"champions\")\n",
    "\n",
    "# players_df.show()\n",
    "title_count=spark.sql(\"\"\"\n",
    "   select player,count(player) as p_count from (\n",
    "    select wimbledon as player from champions union all\n",
    "    select Fr_open from champions union all\n",
    "    select US_open from champions union all\n",
    "    select AUopen from champions\n",
    "    ) group by player\n",
    "\"\"\")\n",
    "\n",
    "title_count.join(players_df,title_count.player==players_df.player_id)\\\n",
    ".select(\"player\",\"player_name\",\"p_count\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "409098cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, login_date: date, diff_days: int, sum_days: bigint]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"𝐂𝐡𝐚𝐥𝐥𝐞𝐧𝐠𝐞:\n",
    "-- Given is user login table for , identify dates where a user has logged in for 5 or more consecutive days.\n",
    "-- Return the user id, start date, end date and no of consecutive days, sorting based on user id.\n",
    "-- If a user logged in consecutively 5 or more times but not spanning 5 days then they should be excluded.\n",
    "\"\"\"\n",
    "\n",
    "data = [\n",
    "    (1, '2024-03-01'),(1, '2024-03-02'),(1, '2024-03-03'),(1, '2024-03-04'),(1, '2024-03-06'),(1, '2024-03-10'),\n",
    "    (1, '2024-03-11'),(1, '2024-03-12'),(1, '2024-03-13'),(1, '2024-03-14'),(1, '2024-03-20'),(1, '2024-03-25'),\n",
    "    (1, '2024-03-26'),(1, '2024-03-27'),(1, '2024-03-28'),(1, '2024-03-29'),(1, '2024-03-30'),(2, '2024-03-01'),\n",
    "    (2, '2024-03-02'),(2, '2024-03-03'),(2, '2024-03-04'),(3, '2024-03-01'),(3, '2024-03-02'),(3, '2024-03-03'),\n",
    "    (3, '2024-03-04'),(3, '2024-03-04'),(3, '2024-03-04'),(3, '2024-03-05'),(4, '2024-03-01'),(4, '2024-03-02'),\n",
    "    (4, '2024-03-03'),(4, '2024-03-04'),(4, '2024-03-04')\n",
    "]\n",
    "from pyspark.sql.functions import col,to_date,lag,date_diff,expr,sum\n",
    "from pyspark.sql.window import Window\n",
    "schema = \"user_id int , login_date string\"\n",
    "diff_window=Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
    "df = spark.createDataFrame(data = data , schema = schema).withColumn(\"login_date\",to_date(\"login_date\",\"yyyy-MM-dd\")).dropDuplicates()\n",
    "get_diff=df.withColumn(\"date_diff\",date_diff(col(\"login_date\"),lag(\"login_date\").over(diff_window)))\n",
    "filter_diff=get_diff.selectExpr(\"user_id\",\"login_date\",\"coalesce(date_diff,0) as diff_days\")\n",
    "filter_ones=filter_diff#.where(expr(\"diff_days <=1\"))\n",
    "\n",
    "sum_window=Window.partitionBy(\"user_id\",\"diff_days\").orderBy(\"login_date\")\n",
    "# filter_ones.printSchema()\n",
    "filter_ones.withColumn(\"sum_days\",sum(col(\"diff_days\")).over(sum_window))\\\n",
    "# .show(n=100)\n",
    "#where(expr(\"date_diff<=1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdb04a3c-f3c7-4f0d-ab52-24b16aafe866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,posexplode,explode_outer\\\n",
    "                                  ,posexplode_outer\n",
    "\n",
    "data = [('Jaya', '20', ['SQL', 'Data Science']),('Rohit', '19', None)\n",
    "        ,('Maria', '20', ['DBMS', 'Networking']),\n",
    "        ('Jay', '22', None),('Milan', '21', ['ML', 'AI'])\n",
    "       ]\n",
    "columns = ['Name', 'Age', 'Courses_enrolled']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.select(df.Name,df.Age,explode(df.Courses_enrolled))\\\n",
    "# .show()\n",
    "\n",
    "df.select(df.Name,df.Age,posexplode(df.Courses_enrolled))\\\n",
    "# .show()\n",
    "\n",
    "df.select(df.Name,df.Age,explode_outer(df.Courses_enrolled))\\\n",
    "# .show()\n",
    "\n",
    "df1=df.select(df.Name,posexplode_outer(df.Courses_enrolled))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3ecdb5be-fb8d-4d5c-946c-0b1aca22a541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with no null values: ['id', 'Name']\n",
      "Columns with All null values: ['salary']\n",
      "Columns with one null values: ['marks', 'salary', 'dept']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identifying missing values and treating/imputing missing values is an important step in exploratory data analysis.\n",
    "Let's try it in the PySpark dataframe API.\n",
    "Task - In a PySpark dataframe,\n",
    "a) Identify columns that contain no null values.\n",
    "b) Identify columns where every value is null.\n",
    "c) Identify columns with at least one null value\n",
    "\"\"\"\n",
    "data = [\n",
    "    (1,'Neha' , 30 , None, 'IT'),\n",
    "    (2,'Mark' , None , None, 'HR'),\n",
    "    (3,'David' , 25 , None, 'HR'),\n",
    "    (4,'Carol' , 30 , None, None)\n",
    "]\n",
    "schema=\"id int,Name string,marks int,salary int,dept string\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "# df.printSchema()\n",
    "# a) Identify columns that contain no null values.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "non_null_columns=[\n",
    "col_name for col_name in df.columns if df.filter(col(col_name).isNull()).count()==0\n",
    "]\n",
    "\n",
    "print(\"Columns with no null values:\", non_null_columns)\n",
    "\n",
    "# b) Identify columns where every value is null.\n",
    "\"\"\"\n",
    "null_columns=[ col_name for col_name in df.columns if df.filter(col(col_name).isNull()).count()==df.count() ]\n",
    "\"\"\"\n",
    "null_columns=[\n",
    "col_name for col_name in df.columns if df.filter(col(col_name).isNotNull()).count()==0\n",
    "]\n",
    "print(\"Columns with All null values:\", null_columns)\n",
    "\n",
    "# c) Identify columns with at least one null value\n",
    "one_null_columns=[\n",
    "col_name for col_name in df.columns if df.filter(col(col_name).isNull()).count()>=1\n",
    "]\n",
    "\n",
    "print(\"Columns with one null values:\", one_null_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b885a6b-4f37-4e16-9a3b-3c0af84f8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You have a row that is too long to show horizontally, also the texts get truncated. \n",
    "How can you show the data vertically, and also show the full text?\n",
    "\"\"\"\n",
    "data = [\n",
    "    (1,'Neha' , 30 , None, 'IT',2,'Mark' , None , None, 'HR'),\n",
    "    (3,'David' , 25 , None, 'HR',4,'Carol' , 30 , None, None)\n",
    "]\n",
    "schema=\"id int,Name string,marks int,salary int,dept string,id2 int,Name2 string,marks2 int,salary2 int,dept2 string\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "# df.show(n=2 ,vertical = True , truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984c0c50-e5c0-4190-8c1e-bac4a38c23b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: string, order_date: date, restaurant: string, city: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i want to return top 3 resturants in the city in the month wise in terms of orders\n",
    "# month,restaurant,city\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [\n",
    "    (1, '2023-01-01', 'Restaurant A', 'New York'),\n",
    "    (2, '2023-01-02', 'Restaurant B', 'Los Angeles'),\n",
    "    (3, '2023-01-03', 'Restaurant C', 'Chicago'),\n",
    "    (4, '2023-01-04', 'Restaurant B', 'Houston'),\n",
    "    (5, '2023-01-05', 'Restaurant C', 'Phoenix'),\n",
    "    (6, '2023-01-06', 'Restaurant C', 'Philadelphia'),\n",
    "    (7, '2023-01-07', 'Restaurant D', 'San Antonio'),\n",
    "    (8, '2023-01-08', 'Restaurant D', 'San Diego'),\n",
    "    (9, '2023-01-09', 'Restaurant A', 'Dallas'),\n",
    "    (10, '2023-01-10', 'Restaurant A', 'Austin'),\n",
    "    (11, '2023-01-11', 'Restaurant E', 'San Jose'),\n",
    "    (12, '2023-01-12', 'Restaurant E', 'Jacksonville'),\n",
    "    (13, '2023-01-13', 'Restaurant D', 'Fort Worth'),\n",
    "    (14, '2023-01-14', 'Restaurant E', 'Columbus'),\n",
    "    (15, '2023-01-15', 'Restaurant D', 'Charlotte')\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"restaurant\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.withColumn(\"order_date\",to_date(\"order_date\",'yyyy-MM-dd'))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3deaeca5-c37c-4731-ad81-dfd1e2c2363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+----------+\n",
      "|ProductCode|Quantity|UnitPrice|CustomerID|\n",
      "+-----------+--------+---------+----------+\n",
      "|       P005|   eight|    12.75|      C002|\n",
      "+-----------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Identify rows containing non-numeric values in the \"Quantity\" column, if any.\n",
    "Detecting non-numeric values within columns intended for numerical data is a valuable step in both data exploration and data cleaning processes.\n",
    "\"\"\"\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"ProductCode\", StringType(), True),\n",
    "StructField(\"Quantity\", StringType(), True),\n",
    "StructField(\"UnitPrice\", StringType(), True),\n",
    "StructField(\"CustomerID\", StringType(), True),\n",
    "])\n",
    "\n",
    "data = [\n",
    "(\"P001\", 5, 20.0, \"C001\"),\n",
    "(\"P002\", 3, 15.5, \"C002\"),\n",
    "(\"P003\", 10, 5.99, \"C003\"),\n",
    "(\"P004\", 2, 50.0, \"C001\"),\n",
    "(\"P005\", \"eight\", 12.75, \"C002\"),\n",
    "]\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "df.select(\"ProductCode\",\"Quantity\",\"UnitPrice\",\"CustomerID\").where(col(\"Quantity\").cast(\"double\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e1d45cd-5ce5-4870-a0e9-5b63675f3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, brand_name: string, seq_no: int]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "𝐓𝐡𝐞 𝐂𝐡𝐚𝐥𝐥𝐞𝐧𝐠𝐞: Given a table with 'brands' and missing 'category' values, can you craft an SQL query that fills those\n",
    "gaps with the last non-null category? (Edit - Lets solve in PySpark)\n",
    "\n",
    "My PySpark dataframe API solution is in the comment (backward fill concept using last()over()).\n",
    "Link to one SQL solution in comment\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data =[('chocolates','5-star'),(None,'dairy milk'),(None,'perk')\n",
    "        ,(None,'eclair'),('Biscuits','britannia'),(None,'good day'),(None,'boost')\n",
    "      ]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"category\" , \"brand_name\"])\n",
    "windowSpec=Window.orderBy(\"seq_no\")#.rowsBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "df=df.withColumn('seq_no',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "df.withColumn(\"category\",last(\"category\",ignorenulls=True).over(windowSpec))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f67ac5ed-d004-46fe-b006-ca01abf15e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: int, job_role: string, skills: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "𝐂𝐡𝐚𝐥𝐥𝐞𝐧𝐠𝐞: Fill the Null values. According to the ouput dataframe.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "job_skills_data = [\n",
    " (1, 'Data Engineer', 'SQL'), (2, None, 'Python'), (3, None, 'AWS'),\n",
    " (4, None, 'Snowflake'), (5, None, 'Apache Spark'), (6, 'Web Developer', 'Java'),\n",
    " (7, None, 'HTML'), (8, None, 'CSS'), (9, 'Data Scientist', 'Python'),\n",
    " (10, None, 'Machine Learning'), (11, None, 'Deep Learning'),(12, None, 'Tableau')\n",
    "]\n",
    "\n",
    "job_skills_schema = \"row_id int , job_role string , skills string\"\n",
    "\n",
    "job_skills_df = spark.createDataFrame(data = job_skills_data , schema = job_skills_schema)\n",
    "windowSpec=Window.orderBy(\"row_id\")\n",
    "job_skills_df.withColumn(\"job_role\",last(\"job_role\",ignorenulls=True).over(windowSpec))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc45bb87-3c5d-451c-9307-e9aa4e47fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 09:40:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[patient_id: bigint, patient_name: string, conditions: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes.\n",
    "Type I Diabetes always starts with the DIAB1 prefix. Return the result table in any order.\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"DiabetesPatients\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "(1, \"Daniel\", \"YFEV COUGH\"),\n",
    "(2, \"Alice\", \"\"),\n",
    "(3, \"Bob\", \"DIAB100 MYOP\"),\n",
    "(4, \"George\", \"ACNE DIAB100\"),\n",
    "(5, \"Alain\", \"DIAB201\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "patients_df = spark.createDataFrame(data, [\"patient_id\", \"patient_name\", \"conditions\"])\n",
    "patients_df.where(\"conditions like '%DIAB1%'\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d25ff7-3330-47df-b43d-313e82ebf3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Department: string, Salary: string, sal_rank: int]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Find the second highest salary in the Finance department.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc,dense_rank\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "StructField(\"Name\", StringType(), True),\n",
    "StructField(\"Age\", IntegerType(), True),\n",
    "StructField(\"Department\", StringType(), True),\n",
    "StructField(\"Salary\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create data\n",
    "data = [(\"Mike\", 30, \"Finance\", \"50,000\"),\n",
    "(\"Jim\", 35, \"Finance\", \"30,000\"),\n",
    "(\"Ami\", 22, \"Finance\", \"30,000\"),\n",
    "(\"Laura\",29, \"Sales\", \"35,000\"),\n",
    "(\"Kia\", 28, \"Sales\", \"30,000\"),\n",
    "(\"Megha\", 22, \"Finance\", \"20,000\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "windowSpec=Window.partitionBy(\"Department\").orderBy(desc(\"Salary\"))\n",
    "df=df.where(\"Department='Finance'\")\n",
    "df.withColumn(\"sal_rank\",dense_rank().over(windowSpec))\\\n",
    ".where(\"sal_rank=2\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8dd1ce-624b-4e2d-b8df-4bfe487f4610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, product: string, date: string, sales: bigint, sal_rank: int, prev_sal: bigint, subsequent_sal: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "🔑 Step 1️⃣: Defining the Challenge\n",
    "Your objective is to analyze the top-performing products in each category, along with their previous and subsequent sales trends.\n",
    "🛠️ Step 2️⃣: PySpark Solution\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Sample sales data\n",
    "data = [(\"Electronics\", \"Laptop\", \"2024-04-01\", 100),\n",
    "(\"Electronics\", \"Laptop\", \"2024-04-02\", 120),\n",
    "(\"Electronics\", \"Laptop\", \"2024-04-03\", 150),\n",
    "(\"Electronics\", \"Phone\", \"2024-04-01\", 200),\n",
    "(\"Electronics\", \"Phone\", \"2024-04-02\", 180),\n",
    "(\"Electronics\", \"Phone\", \"2024-04-03\", 220)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"category\", \"product\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# df.show()\n",
    "\n",
    "# get overall rank of that product,previous price,subsequent ( next) price.\n",
    "windowSpec=Window.partitionBy(\"category\").orderBy(desc(\"sales\"))\n",
    "\n",
    "df.withColumn(\"sal_rank\",dense_rank().over(windowSpec))\\\n",
    ".withColumn(\"prev_sal\",F.lag(\"sales\").over(windowSpec))\\\n",
    ".withColumn(\"subsequent_sal\",F.lead(\"sales\").over(windowSpec))\\\n",
    "# .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
