{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a090f4-fb3d-4be3-9c6b-75a03e831a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file covers basic steps to get started with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce10357-88b1-4ce4-89ab-48f5cec4d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e5ebd5-9516-4a81-98be-f86f4e34aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Read and display data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('first').getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35),(\"Tony\",50)]\n",
    "df=spark.createDataFrame(data,[\"Name\",\"Age\"])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1643b9c0-ae31-4ebb-8832-ba53107af397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "|   Tony| 50|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2 : Filtering Data\n",
    "df.filter(df.Age>30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbda4fe-f506-433f-8caa-13abbbe3eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 25|    1|\n",
      "| 30|    1|\n",
      "| 35|    1|\n",
      "| 50|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task : Grouping and Aggregation\n",
    "df.groupBy(df.Age).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e661-0d51-4f35-8f53-397dbe68fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f1df3d-3e2d-4acc-8733-2deb96a8bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Occupation|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|  Engineer|\n",
      "|    Bob| 30|    Doctor|\n",
      "|Charlie| 35|   Teacher|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task Joining dataframes\n",
    "data2 = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\"), (\"Charlie\", \"Teacher\")]\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Occupation\"])\n",
    "df.join(df2,\"Name\").show()\n",
    "# Alice,(25,Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3fb845-bc02-4f33-9fb2-7701f566543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task : Handling Missing Data or remove the record that contain null value\n",
    "data_with_null = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 35)]\n",
    "df_with_null = spark.createDataFrame(data_with_null, [\"Name\", \"Age\"])\n",
    "df_with_null.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605880-2d7d-4622-84a1-9975bcefa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "994e84d5-7a89-4225-ac68-30d3e83c3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   Name|Age|UpperName|\n",
      "+-------+---+---------+\n",
      "|  Alice| 25|    ALICE|\n",
      "|    Bob| 30|      BOB|\n",
      "|Charlie| 35|  CHARLIE|\n",
      "|   Tony| 50|     TONY|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task: User-Defined Functions (UDFs): \n",
    "# Convert & Create upperCase column for Name column\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "upper_udf=udf(lambda name:name.upper())\n",
    "df.withColumn(\"UpperName\",upper_udf(df[\"Name\"])).show()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f17b39e-2f7c-4eaa-9a0f-173ca08cd0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/02/01 22:39:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   Name|Age|Row_num|\n",
      "+-------+---+-------+\n",
      "|  Alice| 25|      1|\n",
      "|    Bob| 30|      2|\n",
      "|Charlie| 35|      3|\n",
      "|   Tony| 50|      4|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window Functions : generate rownumber based on Age class\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "orderBySpec=Window.orderBy(\"Age\")\n",
    "df.withColumn(\"Row_num\",row_number().over(orderBySpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9e117-7d50-4cc5-b2b1-3e1aef57ac05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364f24e-69fd-469a-a930-5346b4c8df4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
