{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a090f4-fb3d-4be3-9c6b-75a03e831a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file covers basic steps to get started with pyspark\n",
    "# Basic Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a65ee6-181b-4dfa-bd99-0c102eb21040",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 2\u001b[0m spark\u001b[38;5;241m=\u001b[39m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFundementals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\env\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\env\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\env\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\env\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\env\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Fundementals').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1e5ebd5-9516-4a81-98be-f86f4e34aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "|   Tony| 50|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Read and display data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('first').getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35),(\"Tony\",50)]\n",
    "df=spark.createDataFrame(data,[\"Name\",\"Age\"])\n",
    "df.filter(df.Age>30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1643b9c0-ae31-4ebb-8832-ba53107af397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2 : Filtering Data\n",
    "# df.filter(df.Age>30).show()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fbda4fe-f506-433f-8caa-13abbbe3eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task : Grouping and Aggregation\n",
    "# df.groupBy(df.Age).count().show()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122e661-0d51-4f35-8f53-397dbe68fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f1df3d-3e2d-4acc-8733-2deb96a8bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Occupation|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|  Engineer|\n",
      "|    Bob| 30|    Doctor|\n",
      "|Charlie| 35|   Teacher|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+----------+\n",
      "|   Name|Age|   Name|Occupation|\n",
      "+-------+---+-------+----------+\n",
      "|  Alice| 25|  Alice|  Engineer|\n",
      "|    Bob| 30|    Bob|    Doctor|\n",
      "|Charlie| 35|Charlie|   Teacher|\n",
      "|   Tony| 50|   NULL|      NULL|\n",
      "+-------+---+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+----------+\n",
      "|   Name|Age|   Name|Occupation|\n",
      "+-------+---+-------+----------+\n",
      "|  Alice| 25|  Alice|  Engineer|\n",
      "|    Bob| 30|    Bob|    Doctor|\n",
      "|Charlie| 35|Charlie|   Teacher|\n",
      "+-------+---+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+----------+\n",
      "|   Name|Age|   Name|Occupation|\n",
      "+-------+---+-------+----------+\n",
      "|  Alice| 25|  Alice|  Engineer|\n",
      "|    Bob| 30|    Bob|    Doctor|\n",
      "|Charlie| 35|Charlie|   Teacher|\n",
      "|   Tony| 50|   NULL|      NULL|\n",
      "+-------+---+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Tony| 50|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task Joining dataframes\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35),(\"Tony\",50)]\n",
    "data2 = [(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\"), (\"Charlie\", \"Teacher\")]\n",
    "\n",
    "df=spark.createDataFrame(data,[\"Name\",\"Age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Occupation\"])\n",
    "\n",
    "df.join(df2,\"Name\").show()\n",
    "df.join(df2,df.Name==df2.Name,\"left\").show()\n",
    "df.join(df2,df.Name==df2.Name,\"right\").show()\n",
    "df.join(df2,df.Name==df2.Name,\"full\").show()\n",
    "df.join(df2,df.Name==df2.Name,\"leftanti\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3fb845-bc02-4f33-9fb2-7701f566543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task : Handling Missing Data or remove the record that contain null value\n",
    "data_with_null = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 35)]\n",
    "df_with_null = spark.createDataFrame(data_with_null, [\"Name\", \"Age\"])\n",
    "df_with_null.na.drop()\n",
    "# .show()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605880-2d7d-4622-84a1-9975bcefa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994e84d5-7a89-4225-ac68-30d3e83c3480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint, UpperName: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: User-Defined Functions (UDFs): \n",
    "# Convert & Create upperCase column for Name column\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "upper_udf=udf(lambda name:name.upper())\n",
    "df.withColumn(\"UpperName\",upper_udf(df[\"Name\"]))\n",
    "# .show()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f17b39e-2f7c-4eaa-9a0f-173ca08cd0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint, Row_num: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Window Functions : generate rownumber based on Age class\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "orderBySpec=Window.orderBy(\"Age\")\n",
    "df.withColumn(\"Row_num\",row_number().over(orderBySpec)) \\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b9e117-7d50-4cc5-b2b1-3e1aef57ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_name: string, department: string, sum(salary): bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Groupby on multiple columns\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)\n",
    "df.groupBy(\"employee_name\", \"department\").sum(\"salary\")\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6364f24e-69fd-469a-a930-5346b4c8df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstname: string, middlename: string, lastname: string, regno: string, sex: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define nested structType\n",
    "# Defining schema using nested StructType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),None,\"F\",-1)\n",
    "  ]\n",
    "structSchema=StructType([\n",
    "    StructField(\"fullname\",\n",
    "            StructType([\n",
    "                StructField(\"firstname\",StringType(),True),\n",
    "                StructField(\"middlename\",StringType(),True),\n",
    "                StructField(\"lastname\",StringType(),True)\n",
    "    ])),\n",
    "    StructField(\"regno\",StringType(),True),\n",
    "    StructField(\"sex\",StringType(),True),\n",
    "    StructField(\"salary\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=structureData,schema=structSchema)\n",
    "df.select(\"fullname.firstname\",\"fullname.middlename\",\"fullname.lastname\",\"regno\",\"sex\")\\\n",
    "# .show(truncate=False)\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac070764-9e7e-437a-b314-e8e5b32b3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and write into parquet file\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = schema)\n",
    "'''\n",
    "Write the data into different file formats into HDFS Location\n",
    "'''\n",
    "# df.write.parquet(\"output.parquet\", compression=\"snappy\")\n",
    "# df.write.csv('Files/output.csv')\n",
    "df.write.orc('output.orc')\n",
    "df.write.json('output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5acf115-e493-4e88-9e1f-0789424a99f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_name: string, department: string, salary: bigint, Test_Col: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add constant columns into dataframe using lit()\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.select(\"employee_name\", \"department\", \"salary\",lit(\"Salary\").alias(\"Test_Col\"))\\\n",
    "# .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47c987c-8942-46b3-bbf2-91f685d44fec",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2996426486.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    ds = df.as[Person]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learn about dataset\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DatasetExample\").getOrCreate()\n",
    "\n",
    "# Step 2: Define a case class (or schema) to represent the data\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "# Step 3: Create a sequence of data\n",
    "data = [Person(\"Alice\", 25), Person(\"Bob\", 30), Person(\"Charlie\", 22)]\n",
    "\n",
    "# Step 4: Convert the data to RDD of Rows\n",
    "rdd = spark.sparkContext.parallelize(data).map(lambda x: Row(name=x.name, age=x.age))\n",
    "\n",
    "# Step 5: Create a DataFrame from the RDD\n",
    "df = spark.createDataFrame(rdd)\n",
    "\n",
    "# Step 6: Convert the DataFrame to a Dataset using as operator\n",
    "ds = df.as[Person]\n",
    "\n",
    "# Step 7: Show the content of the Dataset\n",
    "ds.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad152d0-1b75-4323-9551-0b52700850dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "what will happen if you insert wrong data into defined schema ?\n",
    "\"\"\"\n",
    "# from pyspark.sql import St\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 1000)\n",
    "  ]\n",
    "schema =StructType([\n",
    "    StructField(\"employee_name\",StringType(),True),\n",
    "    StructField(\"department\",StringType(),True),\n",
    "    StructField(\"salary\",IntegerType(),False),\n",
    "\n",
    "])\n",
    "df = spark.createDataFrame(data=data, schema = schema)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76459cdb-9503-45e7-a5ad-7bf40436554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word count program\n",
    "\"\"\"\n",
    "# from pyspark.sql.functions import split\n",
    "sentences=[(\"Remember to handle credentials securely and be cautious about handling sensitive information like usernames and passwords\",)]\n",
    "df=spark.createDataFrame(sentences,['sentence'])\n",
    "words=(\n",
    "df.selectExpr(\"split(sentence,' ') as words\")\\\n",
    "  .selectExpr(\"explode(words) as word\") \\\n",
    "  .groupBy(\"word\").count()\n",
    ")\n",
    "# df=df.split(' ')\n",
    "# words.show(truncate=False)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82129f25-ed12-4a3a-a7c4-ef896be9d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Remember', 1),\n",
       " ('about', 1),\n",
       " ('and', 2),\n",
       " ('be', 1),\n",
       " ('cautious', 1),\n",
       " ('credentials', 1),\n",
       " ('handle', 1),\n",
       " ('handling', 1),\n",
       " ('information', 1),\n",
       " ('like', 1),\n",
       " ('passwords', 1),\n",
       " ('securely', 1),\n",
       " ('sensitive', 1),\n",
       " ('to', 1),\n",
       " ('usernames', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count using RDD\n",
    "sentences=[\"Remember to handle credentials securely and be cautious about handling sensitive information like usernames and passwords\"]\n",
    "rdd=sc.parallelize(sentences)\n",
    "rdd.flatMap(lambda line:line.split(\" \"))\\\n",
    ".map(lambda word:(word,1))\\\n",
    ".reduceByKey(lambda curr,nxt:curr+nxt)\\\n",
    ".sortBy(lambda word:word[0])\\\n",
    ".collect()\n",
    "# print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff877222-81ff-4335-babb-e315aeda4c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[180] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\"Remember to handle\",\"credentials securely and be cautious\",\"about handling sensitive information like usernames and passwords\"]\n",
    "rdd=sc.parallelize(sentences)\n",
    "rdd.map(lambda word:word.split(\" \"))\\\n",
    ".flatMap(lambda w:w)\\\n",
    "# .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "027df95d-7b1e-40aa-ac80-5bcc33887a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning about format()\n",
    "# df=spark.read.format('csv').option('header',True).load('file:///home/tony/BigData/food_establishment_data.csv')\n",
    "df=spark.read.option('header',True).csv('file:///home/tony/BigData/food_establishment_data.csv')\n",
    "df.write.option('header',True).parquet('file:///home/tony/BigData/food_establishment_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d86c4cb-39a4-4af2-8969-e657a9b85317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266351"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet files count- 266351\n",
    "df=spark.read.option(\"startingFrom\",10)\\\n",
    ".option('header',True)\\\n",
    ".parquet('file:///home/tony/BigData/food_establishment_data.parquet')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "673729ec-d078-457b-b206-391987b19923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with JSON Files\n",
    "# df=spark.read.json('file:///home/tony/BigData/Files/data.json')\n",
    "# rdd=sc.textFile('/home/tony/BigData/Files/data.json')\n",
    "\n",
    "import pandas as pd\n",
    "# df2=pd.DataFrame(eval('/home/tony/BigData/Files/data.json'))\n",
    "data=pd.read_json('/home/tony/BigData/Files/data.json',orient='records')\n",
    "df2=spark.createDataFrame(data)\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82f9f7af-371b-4ff1-993c-83847f7ec20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"file:///home/tony/BigData/Files/data10.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095d33bc-d00c-44b1-94d2-766ecffce6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|user_names|hair_color|eye  |\n",
      "+----------+----------+-----+\n",
      "|James     |black     |brown|\n",
      "|Michael   |brown     |NULL |\n",
      "|Robert    |red       |black|\n",
      "|Washington|grey      |grey |\n",
      "|Jefferson |brown     |     |\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with mapType\n",
    "from pyspark.sql.types import StructType,StructField,StringType,MapType\n",
    "schema=StructType([\n",
    "    StructField(\"user_names\",StringType(),True),\n",
    "    StructField(\"taste\",MapType(StringType(),StringType()),True)\n",
    "])\n",
    "\n",
    "dataDictionary = [\n",
    "    ('James',{'hair':'black','eye':'brown'}),\n",
    "    ('Michael',{'hair':'brown','eye':None}),\n",
    "    ('Robert',{'hair':'red','eye':'black'}),\n",
    "    ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "    ('Jefferson',{'hair':'brown','eye':''})\n",
    "]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "# df.printSchema()\n",
    "df.select(\"user_names\",df.taste['hair'].alias(\"hair_color\"),df.taste['eye'].alias(\"eye\"))\\\n",
    ".show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6b6f4-6973-4030-94a2-31a4e7898961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
