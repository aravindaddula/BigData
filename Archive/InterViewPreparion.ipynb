{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8240c-6b10-428d-8325-cd018bc43043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks perform in this Notebook taken from various platforms like linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a13efaa-887d-4f07-b20f-573642a0ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 19:45:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"GlobalSparkSession\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffca24-1230-4acc-a61e-a831c0c5335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task - The dataset includes company_id, company_name, and multiple date columns representing the stock prices of the company. \n",
    "The task is to generate a new dataset with only three columns, \"company_name\", \"Date\", \"Stock_price\".\n",
    "You have to unpivot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c78978-e557-4a77-8a35-d2105fbc62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "|c_id|        Company_Name|2024-02-01|2024-02-02|2024-02-03|2024-02-04|2024-02-05|\n",
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "|   1| Quantum Innovations|      12.5|      14.5|      16.5|      18.5|      20.5|\n",
      "|   2|   Stellar Solutions|      14.5|      16.5|      18.5|      20.5|      22.5|\n",
      "|   3|     Nebula Dynamics|      16.5|      18.5|      20.5|      22.5|      24.5|\n",
      "|   4|  Fusion Enterprises|      18.5|      20.5|      22.5|      24.5|      26.5|\n",
      "|   5|Celestial Technol...|      20.5|      22.5|      24.5|      26.5|      28.5|\n",
      "+----+--------------------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Quantum Innovations\", 12.5, 14.5, 16.5, 18.5, 20.5),\n",
    "    (2, \"Stellar Solutions\", 14.5, 16.5, 18.5, 20.5, 22.5),\n",
    "    (3, \"Nebula Dynamics\", 16.5, 18.5, 20.5, 22.5, 24.5),\n",
    "    (4, \"Fusion Enterprises\", 18.5, 20.5, 22.5, 24.5, 26.5),\n",
    "    (5, \"Celestial Technologies\", 20.5, 22.5, 24.5, 26.5, 28.5),\n",
    "]\n",
    "columns=[\"c_id\",\"Company_Name\",\"2024-02-01\",\"2024-02-02\",\"2024-02-03\",\"2024-02-04\",\"2024-02-05\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c7f5b0-d8a3-4543-ae63-12a29cd3ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+-----------+\n",
      "|c_id|        Company_Name|     Dates|Stock_Price|\n",
      "+----+--------------------+----------+-----------+\n",
      "|   1| Quantum Innovations|2024-02-01|       12.5|\n",
      "|   1| Quantum Innovations|2024-02-02|       14.5|\n",
      "|   1| Quantum Innovations|2024-02-03|       16.5|\n",
      "|   1| Quantum Innovations|2024-02-04|       18.5|\n",
      "|   1| Quantum Innovations|2024-02-05|       20.5|\n",
      "|   2|   Stellar Solutions|2024-02-01|       14.5|\n",
      "|   2|   Stellar Solutions|2024-02-02|       16.5|\n",
      "|   2|   Stellar Solutions|2024-02-03|       18.5|\n",
      "|   2|   Stellar Solutions|2024-02-04|       20.5|\n",
      "|   2|   Stellar Solutions|2024-02-05|       22.5|\n",
      "|   3|     Nebula Dynamics|2024-02-01|       16.5|\n",
      "|   3|     Nebula Dynamics|2024-02-02|       18.5|\n",
      "|   3|     Nebula Dynamics|2024-02-03|       20.5|\n",
      "|   3|     Nebula Dynamics|2024-02-04|       22.5|\n",
      "|   3|     Nebula Dynamics|2024-02-05|       24.5|\n",
      "|   4|  Fusion Enterprises|2024-02-01|       18.5|\n",
      "|   5|Celestial Technol...|2024-02-01|       20.5|\n",
      "|   4|  Fusion Enterprises|2024-02-02|       20.5|\n",
      "|   5|Celestial Technol...|2024-02-02|       22.5|\n",
      "|   4|  Fusion Enterprises|2024-02-03|       22.5|\n",
      "+----+--------------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "dates_col=[\"Dates\"]\n",
    "dates_data=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df=spark.createDataFrame(dates_data,dates_col)\n",
    "resultset=df.crossJoin(dates_df)\n",
    "final_result=resultset.withColumn(\"Stock_Price\",\n",
    "                      when(col(\"Dates\")=='2024-02-01',col('2024-02-01'))\n",
    "                     .when(col(\"Dates\")=='2024-02-02',col('2024-02-02'))\n",
    "                     .when(col(\"Dates\")=='2024-02-03',col('2024-02-03'))\n",
    "                     .when(col(\"Dates\")=='2024-02-04',col('2024-02-04'))\n",
    "                     .when(col(\"Dates\")=='2024-02-05',col('2024-02-05'))\n",
    "                      )\n",
    "# use sparksql to select required columns\n",
    "final_result.select(\"c_id\",\"Company_Name\",\"Dates\",\"Stock_Price\").show()\n",
    "# dates_df.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43dd18e0-f795-4176-95b7-a81dd614e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the above task using SparkSQL\n",
    "# create temp sql table for values from dates_df dataframe\n",
    "columns2=[\"c_id\",\"Company_Name\",\"feb1\",\"feb2\",\"feb3\",\"feb4\",\"feb5\"]\n",
    "df2=spark.createDataFrame(data,columns2)\n",
    "temp_values=[(\"2024-02-01\",),(\"2024-02-02\",),(\"2024-02-03\",),(\"2024-02-04\",),(\"2024-02-05\",)]\n",
    "dates_df.createOrReplaceTempView(\"temp_values1\")\n",
    "df2.createOrReplaceTempView(\"temp_data1\")\n",
    "# df2.show()\n",
    "results=spark.sql(f\"select c_id,Company_Name,{'Dates'},case Dates when '{'2024-02-01'}' then feb1 when '{'2024-02-02'}' then feb2 when '{'2024-02-03'}' then feb3 when '{'2024-02-04'}' then feb4 else feb5 end as Stock_Price from temp_values1 cross join temp_data1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2712566-a7df-4872-ab4e-9f07ebc8d2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f60f57-cfa7-43e2-b613-e560afab7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results into parque file\n",
    "# results.write.parquet('/home/BigData/Files/stock_price.parquet')\n",
    "results.write.csv('/home/BigData/Files/stock_price.csv')\n",
    "\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "110f85c1-03f6-4204-b049-0c48fa81753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------------------------------+\n",
      "|s_id|s_name|all_books                           |\n",
      "+----+------+------------------------------------+\n",
      "|101 |Mark  |White Tiger;Bhagwad Gita            |\n",
      "|102 |Ria   |The Fountainhead;The Seceret History|\n",
      "|103 |Loi   |The Fountainhead                    |\n",
      "+----+------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task :we have a dataset representing books issued by students, and we want to aggregate the books for each student.\n",
    "# Multiple books should be shown separated by \";\"\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "book_issued = [\n",
    "    (101 ,'Mark' , \"White Tiger\"),\n",
    "    (102 ,'Ria' , \"The Fountainhead\"),\n",
    "    (102 ,'Ria' , \"The Seceret History\"),\n",
    "    (101 ,'Mark' , \"Bhagwad Gita\"),\n",
    "    (103 ,'Loi' , \"The Fountainhead\"),\n",
    "    ]\n",
    "books_schema=['s_id','s_name','book_names']\n",
    "books_df=spark.createDataFrame(book_issued,books_schema)\n",
    "gp=books_df.groupBy(\"s_id\",\"s_name\")\\\n",
    ".agg(F.concat_ws(';',F.collect_list(\"book_names\")).alias(\"all_books\"))\\\n",
    ".show(truncate=False)\n",
    "# print(type(gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01fdab6-07c9-4a0d-afb2-df9377968f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+\n",
      "| id|        Name|Amount|\n",
      "+---+------------+------+\n",
      "|  1|  Aditya Sen|  NULL|\n",
      "|  4|  D K aditya|  NULL|\n",
      "|  3|      Mark T|   100|\n",
      "|  2|Bikramaditya|   200|\n",
      "|  6|         Eli|   300|\n",
      "|  5|       Danny|   500|\n",
      "+---+------------+------+\n",
      "\n",
      "+---+------------+------+\n",
      "| id|        Name|Amount|\n",
      "+---+------------+------+\n",
      "|  5|       Danny|   500|\n",
      "|  6|         Eli|   300|\n",
      "|  2|Bikramaditya|   200|\n",
      "|  3|      Mark T|   100|\n",
      "|  1|  Aditya Sen|  NULL|\n",
      "|  4|  D K aditya|  NULL|\n",
      "+---+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.'Arranging a column with null values in ascending or descending order.'\n",
    "\n",
    "Task - Sort the salary column of the dataframe :\n",
    "\n",
    "'''\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , None),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" ,None),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 300),\n",
    "    ]\n",
    "cust_schema=StructType([\n",
    "    StructField(\"id\",IntegerType(),False), #is Nullable\n",
    "    StructField(\"Name\",StringType(),False),\n",
    "    StructField(\"Amount\",IntegerType(),True)\n",
    "])\n",
    "cust_df=spark.createDataFrame(data,cust_schema)\n",
    "# print(cust_df)\n",
    "# cust_df.show()\n",
    "# a) in ascending order with NULL values at the top\n",
    "cust_df.orderBy(\"Amount\").show()\n",
    "# b) in ascending order with NULL values at the bottom\n",
    "\n",
    "# c) in descending order with NULL values at the top\n",
    "\n",
    "# d) in descending order with NULL values at the bottom\n",
    "cust_df.orderBy(F.desc(\"Amount\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a45e1c-b50e-4950-b4e5-ae0972b0520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|Order_id|         Order_date|Order_date2|\n",
      "+--------+-------------------+-----------+\n",
      "|       1|2024-02-01 00:00:00| 2024-02-01|\n",
      "|       2|2024-02-01 00:00:00| 2024-02-01|\n",
      "|       3|2024-02-03 00:00:00| 2024-02-03|\n",
      "|       4|2024-02-04 00:00:00| 2024-02-04|\n",
      "+--------+-------------------+-----------+\n",
      "\n",
      "+--------+----------+\n",
      "|Order_id|order_date|\n",
      "+--------+----------+\n",
      "|       1|2024-02-01|\n",
      "|       2|2024-02-01|\n",
      "|       3|2024-02-03|\n",
      "|       4|2024-02-04|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task - In your dataset, the \"order_date\" column includes a time component set to 00:00:00.\n",
    "Your objective is to eliminate the time portion from the column, thereby conserving storage space.\n",
    "'Remove time part from PySpark dataframe date column'\n",
    "'''\n",
    "from pyspark.sql.functions import col\n",
    "data = [(1, \"2024-02-01 00:00:00\"),\n",
    "        (2, \"2024-02-01 00:00:00\"),\n",
    "        (3, \"2024-02-03 00:00:00\"),\n",
    "        (4, \"2024-02-04 00:00:00\",)]\n",
    "columns = [\"Order_id\",\"Order_date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "# using sparksql\n",
    "df.createOrReplaceTempView(\"orders\")\n",
    "# print(df)\n",
    "removedTime = spark.sql(\"select Order_id,cast(substring(Order_date,1,10) as date) as order_date from orders\")\n",
    "# print(removedTime)\n",
    "\n",
    "# using Pyspark\n",
    "df.withColumn(\"Order_date2\",col(\"Order_date\")[0:10]).show()\n",
    "removedTime.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4aebf4-eebe-4eef-8a53-9d21bfd7d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''case insensitive/ignorant data search'\n",
    "Task - In the provided dataset, identify the rows where the 'Name' column contains the word 'aditya'.\n",
    "'''\n",
    "from pyspark.sql.functions import filter\n",
    "\n",
    "data = [\n",
    "    (1, \"Aditya Sen\" , 100),\n",
    "    (2, \"Bikramaditya\" , 200),\n",
    "    (3, \"Mark T\" , 100),\n",
    "    (4, \"D K aditya\" , 200),\n",
    "    (5, \"Danny\" , 500),\n",
    "    (6, \"Eli\" , 100),\n",
    "]\n",
    "columns = [\"id\",\"name\",\"Amount\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "# spark.sql(\"select id,name,Amount from users where lower(name) like '%aditya%'\").show()\n",
    "# df.where(\"lower(name) like '%aditya%'\").show()\n",
    "# df.filter(df[\"name\"]).rlike('aditya').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca43df11-b586-4757-9fa1-c6b2ed8c3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+----------+\n",
      "|              Type|    Department|Avg_Salary|\n",
      "+------------------+--------------+----------+\n",
      "|Full-time Employee|Administration|   15000.0|\n",
      "|            Intern|     Technical|    5000.0|\n",
      "|Full-time Employee|    Management|   20000.0|\n",
      "|Part-time Employee|     Technical|   10000.0|\n",
      "+------------------+--------------+----------+\n",
      "\n",
      "+------------------+--------------+----------+---------+\n",
      "|              Type|Administration|Management|Technical|\n",
      "+------------------+--------------+----------+---------+\n",
      "|Full-time Employee|       15000.0|   20000.0|      0.0|\n",
      "|            Intern|           0.0|       0.0|   5000.0|\n",
      "|Part-time Employee|           0.0|       0.0|  10000.0|\n",
      "+------------------+--------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task - Show work group wise , average salary for each department\n",
    "# Suggestion: instead of following sql like approach follow, spark approach\n",
    "\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,when,avg\n",
    "import pandas as pd\n",
    "spark = spark.builder.getOrCreate()\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"FirstName\",ArrayType(StringType()),True),\n",
    "    StructField(\"LastName\",ArrayType(StringType()),True),\n",
    "    StructField(\"Type\",ArrayType(StringType()),True),\n",
    "    StructField(\"Dept\",ArrayType(StringType()),True),\n",
    "    StructField(\"Yoe\",ArrayType(IntegerType()),True),\n",
    "    StructField(\"Sal\",ArrayType(IntegerType()),True)\n",
    "])\n",
    "data = {'First Name': ['Aryan', 'Rohan', 'Riya', 'Yash', 'Siddhant'],\n",
    "        'Last Name': ['Singh', 'Agarwal', 'Shah', 'Bhatia', 'Khanna'],\n",
    "        'Type': ['Full-time Employee', 'Intern', 'Full-time Employee', 'Part-time Employee', 'Full-time Employee'],\n",
    "        'Department': ['Administration', 'Technical', 'Administration', 'Technical', 'Management'],\n",
    "        'YoE': [2, 3, 5, 7, 6],\n",
    "        'Salary': [20000, 5000, 10000, 10000, 20000]\n",
    "       }\n",
    "\n",
    "pandas_df=pd.DataFrame(data)\n",
    "# print(pandas_df)\n",
    "# data1=(list(data.values()), list(data.keys()))\n",
    "# df = spark.createDataFrame(list(data.values()), list(data.keys()))\n",
    "\n",
    "df=spark.createDataFrame(pandas_df);\n",
    "grouped_data=df.groupBy(\"Type\",\"Department\").agg(avg('Salary').alias(\"Avg_Salary\"))#,min(\"Department\").alias(\"Department\"))\n",
    "grouped_data.show()\n",
    "'''\n",
    "gd=grouped_data.withColumn(\"Administration\",when(col(\"Department\")=='Administration',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Technical\",when(col(\"Department\")=='Technical',col('Avg_Salary')).otherwise(0))\\\n",
    "            .withColumn(\"Management\",when(col(\"Department\")=='Management',col('Avg_Salary')).otherwise(0))\n",
    "# gd.select(\"Type\",\"Administration\",\"Management\",\"Technical\").show()\n",
    "'''\n",
    "df.groupBy(\"Type\").pivot(\"Department\").avg(\"Salary\").na.fill(0).alias(\"Avg_Sal\").show()\n",
    "\n",
    "# gd.show()\n",
    "# print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6ff6af-7f54-415c-b14b-dee5ba2adcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+\n",
      "| Branch|Student|Maths_marks|Science_marks|\n",
      "+-------+-------+-----------+-------------+\n",
      "|  Delhi|   Neha|         90|        -9999|\n",
      "|Kolkata|   Arav|         83|           79|\n",
      "|Kolkata|Unknown|         73|           89|\n",
      "+-------+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task - You are provided with two datasets, branch1 and branch2 , representing information about students and\n",
    "their marks in different subjects across different branches. Your goal is to combine these datasets into one final dataset.\n",
    "Missing text information should be shown as 'unknown' , and missing numerical information should be shown as -9999.\n",
    "'''\n",
    "from pyspark.sql.functions import lit\n",
    "branch1 = spark.createDataFrame([[\"Delhi\", \"Neha\", 90]], [\"Branch\", \"Student\", \"Maths_marks\"])\n",
    "branch2 = spark.createDataFrame([[\"Arav\",\"Kolkata\", 79, 83], [None,\"Kolkata\", 89, 73]],[\"Student\", \"Branch\", \"Science_marks\", \"Maths_marks\"])\n",
    "\n",
    "branch_v1=branch1.withColumn(\"Science_marks\",lit(-9999)).select(\"Student\",\"Branch\",\"Science_marks\",\"Maths_marks\")\n",
    "union_data=branch_v1.union(branch2).selectExpr(\"Branch\",\"case when Student is null then 'Unknown' else Student end as Student\",\"Maths_marks\",\"Science_marks\")\n",
    "union_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fd454d-b118-41bf-b16e-56f00af8a344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[orders: string, count: bigint]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Task - The dataset contains information about food items in bills.\n",
    "Your assignment is to determine the frequency of each food item ordered.\n",
    "'''\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,ArrayType,StringType\n",
    "from pyspark.sql.functions import explode,count\n",
    "import pandas as pd\n",
    "data = [\n",
    "(101, [\"dosa\", \"biriyani\", \"idli\"]),\n",
    "(102, [\"biriyani\", \"mineral water\"]),\n",
    "(103, [\"rice\", \"mineral water\", \"poha\"]),\n",
    "(109, [\"idli\", \"biriyani\", \"poha\"]),\n",
    "]\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"orders\",ArrayType(StringType()),True)\n",
    "])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).select(explode(\"orders\").alias(\"orders\"))\n",
    "getCount=df.groupBy(\"orders\").count()\n",
    "getCount.orderBy([\"count\"],ascending=False)#.show(truncate=False)\n",
    "# exploded=df.withColumn(\"items\",explode(df.orders))\n",
    "# exploded.show(truncate=False)\n",
    "# orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d173b71b-8fb0-4c06-a93a-23ce87eb3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       orders|count|\n",
      "+-------------+-----+\n",
      "|     biriyani|    3|\n",
      "|         idli|    2|\n",
      "|mineral water|    2|\n",
      "|         poha|    2|\n",
      "|         dosa|    1|\n",
      "|         rice|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized code for the above\n",
    "\n",
    "df=spark.createDataFrame(data,schema).select(explode(\"orders\").alias(\"orders\")) \\\n",
    ".groupBy(\"orders\") \\\n",
    ".count().orderBy(\"count\",ascending=False) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f10600e-2bb3-4f29-9f4e-5aaf68049fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| Name| Sub|Marks|\n",
      "+-----+----+-----+\n",
      "|Rudra|math|   79|\n",
      "|Rudra| eng|   60|\n",
      "|Shivu|math|   68|\n",
      "|Shivu| eng|   59|\n",
      "|  Anu|math|   65|\n",
      "|  Anu| eng|   80|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task - Transform the input DataFrame into a new DataFrame , where each row represents a unique student\n",
    "and the columns include the student's name along with the marks for the \"math\" and \"eng\" subjects.\n",
    "'''\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,ArrayType,StringType\n",
    "\n",
    "data=[\n",
    "('Rudra','math',79),\n",
    "('Rudra','eng',60),\n",
    "('Shivu','math', 68),\n",
    "('Shivu','eng', 59),\n",
    "('Anu','math', 65),\n",
    "('Anu','eng',80)\n",
    "]\n",
    "schema=StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Sub\",StringType(),True),\n",
    "    StructField(\"Marks\",IntegerType(),True)\n",
    "])\n",
    "df=spark.createDataFrame(data,schema)\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f950b6-2824-45ff-bfdd-d22109dcf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Read a table data from the sources like SQL Server/Hive/datalake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5a5ff7b-63ea-4449-a1c8-f87d9bf67738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|product|sum_sal|\n",
      "+-------+-------+\n",
      "| Widget|   1500|\n",
      "| Gadget|   1350|\n",
      "| Doodad|   1200|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|product|sum_sal|\n",
      "+-------+-------+\n",
      "| Widget|   1500|\n",
      "| Gadget|   1350|\n",
      "| Doodad|   1200|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Question: Given a list of sales transactions with product names and their corresponding sales amounts,\n",
    "calculate the total sales for each product and store the results in a dictionary.\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "spark=SparkSession.builder.appName(\"interview_prep\").getOrCreate()\n",
    "sales = [(\"Widget\", 500), (\"Gadget\", 750), (\"Widget\", 1000), (\"Doodad\", 1200), (\"Gadget\", 600)]\n",
    "sales_df=spark.createDataFrame(sales,schema=[\"product\",\"price\"])\n",
    "sales_df=sales_df.groupBy(\"product\").agg(sum(\"price\").alias(\"sum_sal\"))\n",
    "sales_df.show()\n",
    "newdf=sales_df.select(\"product\",\"sum_sal\")\n",
    "newdf.show()\n",
    "#.\\\n",
    "# product_sales_dict = sales_df.to_dict()\n",
    "# print(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6afc5ed-d711-4183-b1b5-f010d62191de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Doodad': 1200, 'Widget': 1500, 'Gadget': 1350}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# above problem using RDD\n",
    "sales = [(\"Widget\", 500), (\"Gadget\", 750), (\"Widget\", 1000), (\"Doodad\", 1200), (\"Gadget\", 600)]\n",
    "rdd=sc.parallelize(sales)\n",
    "\n",
    "# you have to give column index because RDD dont have column names.\n",
    "# any transformation returns pipelined RDD\n",
    "groupData=rdd.reduceByKey(lambda x,y:x+y).collectAsMap()\n",
    "# data=groupData.collect()\n",
    "print(groupData)\n",
    "\n",
    "# print(type(data))\n",
    "# print(groupData)\n",
    "# unpacked_data = {key: list(value) for key, value in groupData.items()}\n",
    "# print(unpacked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d81ee3-b566-4ee3-812e-0c33b1d003da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ggg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
