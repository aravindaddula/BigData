1. Split column name as first name, middle_name,last name
Employee_Name
Darsh, Gupta
Amit ,Kumar,Gupta
Alok ,Gupta
Vijay, Kumar ,gupta


df.select(split(Employee_Name,',',1).alias("First_name"), split(Employee_Name,',',2).alias("middle_name"),
split(Employee_Name,',',3).alias("last_name"))


---------------
2. dynamically pivot all the countries data. do not hardcode values.

+-------+------+-------+
|Product|Amount|Country|
+-------+------+-------+
| Banana|  1000|    USA|
|Carrots|  1500|    USA|
|  Beans|  1600|    USA|
| Orange|  2000|    USA|
| Orange|  2000|    USA|
| Banana|   400|  China|
|Carrots|  1200|  China|
|  Beans|  1500|  China|
| Orange|  4000|  China|
| Banana|  2000| Canada|
|Carrots|  2000| Canada|
|  Beans|  2000| Mexico|
+-------+-----+-------+


output:
+-------+------+-----+------+----+
|Product|Canada|China|Mexico| USA|
+-------+------+-----+------+----+
| Orange|  null| 4000|  null|4000|
|  Beans|  null| 1500|  2000|1600|
| Banana|  2000|  400|  null|1000|
|Carrots|  2000| 1200|  null|1500|


sum_by_cnt = df.groupBy("Product","Country").sum("Amount")

Pdf = sum_by_cnt.withColumn("Canada",when(col("Country")=='Canada"),col("Amount))\
.withColumn("China",when(col("Country")=='China"),col("Amount)) \
.withColumn("Mexico",when(col("Country")=='Mexico"),col("Amount)) \
.withColumn("USA",when(col("Country")=='USA"),col("Amount))

pdf.select("Product","Canada","China","Mexico","USA")

---------------------------------------------

3.From the doctors table, fetch the details of doctors who work in the same hospital but in different speciality.


--Table Structure:

id name speciality hospital city consultation_fee

(1,'Dr. Shashank','Ayurveda','Apollo Hospital','Bangalore', 2500),
(2,'Dr. Abdul','Homeopathy','Fortis Hospital','Bangalore', 2000),
(3,'Dr. Shwetha','Homeopathy','KMC Hospital','Manipal', 1000),
(4,'Dr. Murphy','Dermatology','KMC Hospital','Manipal', 1500),
(5,'Dr. Farhana','Physician','Gleneagles Hospital','Bangalore', 1700),
(6,'Dr. Maryam','Physician','Gleneagles Hospital','Bangalore',1500),


select
	d1.name,d1.speciality,d1.hospital,d1.city,d1,consultation_fee

doctors as d1
inner join doctors as d2
on d1.hospital = d2.hospital and d1.speciality != d2.speciality


-------------------------Second interview - 27-04-2025

1.tell me about your self
2.what is your dataware house design, how your data consumes
3.what type of source is that is that API source
4.are you taking data from outside of AWS
5.in data migration service How did you paln to migrate the data from postgres to s3 
  when the size of the data is very huge with minimal downtime.
6.are you using any ETL Tool , how you are moving the data from s3 to redshift
To copy the data from redshift what are the things required
what is ARN
what is dag in airflow
optimization techniques used in pyspark
what is the difference between broadcast join and broadcast variable
have you used repartition and coalesce
what is the data catalog in glue
are you following agile methodology tell me how it works.
in agile methodology, how to get no.of days required for task
how do you do the ci/cd in your project
how much you have worked on the development part and support part
do you have any design experirience
if you have to do data modeling from where you will start
do you know about normalization and types
what does it mean if a dataset is in dataset format

he is using these tools : ec2,etl,glue,lambda,rds,redshift

------Coding Questions---------------------------------------
1.
def extendList(val, list=[]):
    list.append(val)
    return list
 
 
list1 = extendList(10)
list2 = extendList(123,[])
list3 = extendList('a')
 
 
print "list1 = %s" % list1
print "list2 = %s" % list2
print "list3 = %s" % list3


Output:
[10,'a'] # default list
[123]
[10,'a']

---------------

def multipliers():
  return [lambda x : i * x for i in range(4)]
print [m(2) for m in multipliers()]

Output:
[0,2,4,6]
[2,4,6]

-----------------------



A
----
1
1
Null
0
Null
2
3
 
B
----
1
1
1
Null
0
Null
4
5
7

-------
inner - 7
left - 7 + 3 + 1 =11
full = 7 + 4 + 5=16
7*9=63
left anti-- 3
left semi 7


-----------Deloitte may 23rd

1. give me pyspark code to extract only incremental records

1. we gave you a plane aws account and list the services you use and list the steps to migrate the data from sqlserver to redshift 

sqlserver --> s3 --> redshift

services : iam,glue,s3,redshift,ec2

1.iam role,from iam user access,secret key with glueServiceRole access
2.create bucket (pub access)
3.ec2 with 64gb ram


2.difference between views,materialized views ? when session ends ?
3.list dataware housing concepts
4.union and union all the datasets




delta-core_2.12-3.1.0.jar
delta-storage-3.1.0.jar
